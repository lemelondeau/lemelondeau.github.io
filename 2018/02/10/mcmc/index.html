<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/duola.jpg?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/duola.jpg?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/duola.jpg?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="algorithms," />










<meta name="description" content="1. Sample data from a distributionSometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among al">
<meta name="keywords" content="algorithms">
<meta property="og:type" content="article">
<meta property="og:title" content="Monte Carlo Sampling">
<meta property="og:url" content="https://lemelondeau.github.io/2018/02/10/mcmc/index.html">
<meta property="og:site_name" content="Cool Site">
<meta property="og:description" content="1. Sample data from a distributionSometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among al">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-02-10T15:09:44.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Monte Carlo Sampling">
<meta name="twitter:description" content="1. Sample data from a distributionSometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among al">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lemelondeau.github.io/2018/02/10/mcmc/"/>





  <title>Monte Carlo Sampling | Cool Site</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-80781234-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?//hm.baidu.com/hm.js?ee75cf111111aa99f8540efa2570970";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cool Site</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lemelondeau.github.io/2018/02/10/mcmc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lemelondeau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/duola.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cool Site">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Monte Carlo Sampling</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-10T14:44:53+08:00">
                2018-02-10
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/10/mcmc/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/02/10/mcmc/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-Sample-data-from-a-distribution"><a href="#1-Sample-data-from-a-distribution" class="headerlink" title="1. Sample data from a distribution"></a>1. Sample data from a distribution</h2><p>Sometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among all the distributions, the easiest one to sample from is uniform distribution $U(0,1)$. One thing worth noticing is most of the random generators generate pseudo random numbers rather than genuinely ones. Two commonly used methods are <strong><em>linear congruential generator</em></strong> (LCG) and <strong><em>multiplicative congruential generator</em></strong> (MCG):</p>
<script type="math/tex; mode=display">
\text{LCG:}\left\{
      \begin{array}{ll}
      x_i=(C+\lambda x_{i-1} ) \text{mod}  M \\
      r_i=x_i/M
      \end{array}
\right.</script><script type="math/tex; mode=display">
\text{MCG:}\left\{
      \begin{array}{ll}
      x_i=\lambda x_{i-1} \text{mod} M\\
      r_i=x_i/M
      \end{array}
\right.</script><p>where $x_0$ is called the seed, $C$ is a non-negative integer and $r_i$ is the generated random number. For further information, please refer to <a href="http://statweb.stanford.edu/~owen/mc/Ch-unifrng.pdf" target="_blank" rel="external">this</a>.</p>
<p>Now we have the random numbers from $U(0,1)$, how about other distributions? One well known method is <strong><em>inversion sampling</em></strong>.</p>
<p>Given a distribution $P(X)$, here $X$ is a continuous variable, we know that the output range of its cumulative distribution function $C(X)$ is $[0, 1]$. If we sample a random number $u$ from $U(0, 1)$, there will be always an $x$ such that $C(x)=u$. Now, $x$ is a sample from $P(X)$. To compute $x$ from $u$, we need the inverse function $C^{-1}$.</p>
<script type="math/tex; mode=display">
x=C^{-1}(u)</script><p>What if $F(X)​$ is not invertible or we don’t even have the formula of $P(X)​$? </p>
<h2 id="2-Monte-Carlo-sampling"><a href="#2-Monte-Carlo-sampling" class="headerlink" title="2. Monte Carlo sampling"></a>2. Monte Carlo sampling</h2><p>In this part, we assume $p(x)$ is known.</p>
<h3 id="2-1-Rejection-sampling"><a href="#2-1-Rejection-sampling" class="headerlink" title="2.1 Rejection sampling"></a>2.1 Rejection sampling</h3><p>We want to sample data from a distribution $p(x)$, however, it is very difficult. Suppose we are lucky to have an easy-to-sample distribution $q(x)$, such that $kq(x) \geq p(x), k&gt;0$. $q(x)$ is called proposal distribution. What we are going to do is first sample a data $x_0$ from $q(x)$, and then sample $u_0$ from $U(0, kq(x_0))$. If $u_0&gt;p(x_0)$, the sample is rejected. This method is useful in one or two dimensions. [1]</p>
<h3 id="2-2-Importance-sampling"><a href="#2-2-Importance-sampling" class="headerlink" title="2.2 Importance sampling"></a>2.2 Importance sampling</h3><p>Suppose we have an easy-to-sample proposal distribution $q(x)$, such that $q(x)&gt;0$ if $p(x)&gt;0$. Now, we want to get the expectation of $f(x)$.</p>
<script type="math/tex; mode=display">
\begin{align}
E[f]&=\int f(x)p(x)\text{d}x\\
&=\int f(x)\frac{p(x)}{q(x)}q(x)\text{d}x\\
&\approx \frac1N\sum_n\frac{p(x_n)}{q(x_n)}q(x_n), x_n \sim q(x)
\end{align}</script><p>$w_n=\frac{p(x_n)}{q(x_n)}$ is called <strong>importance weight</strong>. The above formula means sampling $f(x)$ from $p(x)$ is equivalent to sampling $f(x)w(x)$ from $q(x)$. All samples are retained, which is different from the reject sampling.</p>
<h3 id="2-3-Metropolis-Sampling"><a href="#2-3-Metropolis-Sampling" class="headerlink" title="2.3 Metropolis Sampling"></a>2.3 Metropolis Sampling</h3><p>The idea of Metropolis sampling is quite simple: from an initial position $x_0$, apply random walks to propose the next position and decide whether to accept it. If the new position $x_{t+1}$ is more likely to be visited than $x_t$ under $p(x)$, that is $p(x_{t+1})\geq p(x_t)$, the proposed position is accepted. Otherwise, accept the proposed position with probability $p(x_{t+1})/p(x_t)$, set $x_{t+1}$ to $x_t$. For the random walks, a Gaussian distribution is usually used: $p(x_{t+1}|x_t)\sim \mathcal{N}(x_t, 1)$.</p>
<p>I really hate some of the posts given the M-H algorithm (see section 2.4) first and say Metropolis sampling is a special case of it. Especially when they give the acceptance rate without further explanation, which makes me feel like I am the most stupid person in the world. </p>
<p>FYI: The Metropolis algorithm was first proposed in 1953. It was then generalized by Hastings in 1970. [2]</p>
<h3 id="2-4-Markov-Chain-Monte-Carlo-MCMC"><a href="#2-4-Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="2.4 Markov Chain Monte Carlo (MCMC)"></a>2.4 Markov Chain Monte Carlo (MCMC)</h3><p>What is not so good with Metropolis sampling? What is the intuition of MCMC?</p>
<h3 id="2-4-1-Markov-Chain"><a href="#2-4-1-Markov-Chain" class="headerlink" title="2.4.1 Markov Chain"></a>2.4.1 Markov Chain</h3><p>For Markov Chain, there are $n$ states $x_1, x_2, … ,x_n$ and the state at time $t+1$ is conditional independent to other historical states given the state at time $t$</p>
<script type="math/tex; mode=display">
p(x_{t+1}|x_1, x_2,...,x_t)=p(x_{t+1}|x_t)</script><p>$p$ is called transition probability. And the transition function $T$:</p>
<script type="math/tex; mode=display">
T(x_{t+1}\leftarrow x_t) \equiv 
p(x_{t+1}|x_t)</script><p>Starting form a certain state, we can get the distribution $\pi_t(x)$ at any time $t$ using the transition function. We call $\pi(x)$ a <strong>stationary distribution</strong> when it doesn’t change any more:</p>
<script type="math/tex; mode=display">
\pi(x)=\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)</script><p>To make $\pi(x)$ stationary, a sufficient but not necessary condition is</p>
<script type="math/tex; mode=display">
\pi(x^\prime)T(x\leftarrow x^\prime)=\pi(x)T(x^\prime \leftarrow x)</script><p>The above property is called <strong>detailed balance</strong> and we can prove the stationarity with this property:</p>
<script type="math/tex; mode=display">
\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)=\sum_{x^\prime}\pi(x)T(x^\prime\leftarrow x)=\pi(x)</script><h3 id="2-4-2-Metropolis-Hastings-M-H-Algorithm"><a href="#2-4-2-Metropolis-Hastings-M-H-Algorithm" class="headerlink" title="2.4.2 Metropolis-Hastings (M-H) Algorithm"></a>2.4.2 Metropolis-Hastings (M-H) Algorithm</h3><p>M-H algorithm is one of the classical MCMC methods. In this section, we will see how to apply Markov Chain, especially it’s stationary property, to do sampling.</p>
<p><strong>A Markov chain view of Metropolis sampling</strong></p>
<p>Let’s say now we want to sampling from $\pi(x)$, we know that when it is stationary the next position $x_{t+1}$ we get by applying the transition function always has distribution $\pi(x)$. That means we can take $x_{t+1}$ as a sample of $\pi(x)$ ! Wow!</p>
<p>Now our problem is that we don’t know the exact $T$. What can we do?</p>
<p>Let’s look back at the Metropolis method in a Markov chain way. At position $x_t$, we want to know where will the next position be. In Markov chain, we get $x_{t+1}$ using $T(x_{t+1}\leftarrow x_t)$. In Metropolis method, there are two steps, first propose a new position using $Q(x_{t+1}\leftarrow x_t)$ and then decide whether to accept it using $A(x_{t+1}\leftarrow x_t)$. It’s now obvious that</p>
<script type="math/tex; mode=display">
T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)</script><p>And if $x_{t+1}=x_t$, there are two parts, one if the proposal is itself and another is when the proposal is rejected:</p>
<script type="math/tex; mode=display">
T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)+\sum_{x^\prime}Q(x^\prime\leftarrow x_t)(1-A(x^\prime\leftarrow x_t))</script><p>We can specify the proposal distribution, what we don’t have is the acceptance rate $A(x_{t+1}\leftarrow x_t)$. How can we get it?</p>
<p><strong>Acceptance rate</strong></p>
<p>We hope that the chosen acceptance rate will give us a stationary distribution such that the states produced by the Markov chain are from the distribution $\pi(x)$. Applying the <strong>detailed balance</strong> we will get</p>
<script type="math/tex; mode=display">
\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)=\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)</script><p>Subsequently, we can get</p>
<script type="math/tex; mode=display">
%\frac{A(x^\prime \leftarrow x)}{A(x\leftarrow x^\prime)}=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}
\\
A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}A(x\leftarrow x^\prime)</script><p>Ideally, we want the acceptance rate to be <strong>as high as possible</strong>, so we choose $A(x\leftarrow x^\prime)$ to be 1 and $A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}$. But at the meanwhile, it is not larger than 1. So the final result would be</p>
<script type="math/tex; mode=display">
A(x^\prime \leftarrow x)=\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)</script><p>We can prove the stationarity:</p>
<script type="math/tex; mode=display">
\begin{align}
\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)&=\pi(x)Q(x^\prime\leftarrow x)\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)\\
&=\min(\pi(x^\prime)Q(x\leftarrow x^\prime), \pi(x)Q(x^\prime\leftarrow x)) \\
&=\pi(x^\prime)Q(x\leftarrow x^\prime)\min(1, \frac{\pi(x)Q(x^\prime \leftarrow x)}{\pi(x^\prime)Q(x\leftarrow x^\prime)})\\
&=\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)
\end{align}</script><p>OK, now we can see why it is said Metropolis sampling is a special case of M-H algorithm. When $Q(x^\prime \leftarrow x)=Q(x \leftarrow x^\prime)$, M-H algorithm becomes Metropolis sampling.</p>
<p><strong>The algorithm</strong></p>
<ol>
<li>start from $x_0$</li>
<li>propose candidate $x^\prime$ position according to some proposal distribution $p(x^\prime|x)$</li>
<li>accept the candidate with probability $\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)$</li>
<li>if accepted, set $x_{t+1}=x^\prime$, otherwise $x_{t+1}=x_{t}$</li>
</ol>
<p><strong>Burn-in</strong></p>
<p>If the starting position is far from the dense area, it will take some time for the algorithm to reach the dense area (more representative of the distribution). Therefore, burn-in is needed.</p>
<p>Two good videos are <a href="https://www.youtube.com/watch?v=sK3cg15g8FI&amp;list=LLT7nvYvSbuz8tuy5ZqcCVYQ" target="_blank" rel="external">Machine learning - Markov chain Monte Carlo (MCMC) II</a> and<a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/hnzut/metropolis-hastings" target="_blank" rel="external">Metropolis-Hastings</a>.</p>
<h2 id="3-Approximate-Inference"><a href="#3-Approximate-Inference" class="headerlink" title="3. Approximate Inference"></a>3. Approximate Inference</h2><p>In practice, we usually want to sample from a distribution whose formula is unknown. The most common one is posterior $p(\theta|D)$.</p>
<script type="math/tex; mode=display">
p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}</script><p>Usually, $p(D)=\int p(D|\theta)p(\theta)\text{d}\theta $ is intractable, how are we going to deal with this?</p>
<p>Notice that $p(D)$ is a constant and we are taking division so it doesn’t matter what it would be! This makes MCMC a powerful method for approximating posterior distributions.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>​    [1]. <a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf" target="_blank" rel="external">http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf</a></p>
<p>​    [2]. <a href="https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\" target="_blank" rel="external">https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\</a></p>
<p>​    [3]. <a href="http://www.maths.nuigalway.ie/~dane/Friel.pdf" target="_blank" rel="external">http://www.maths.nuigalway.ie/~dane/Friel.pdf</a></p>
<p>​    </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/algorithms/" rel="tag"># algorithms</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/28/SGD/" rel="next" title="Stochastic Gradient Descent">
                <i class="fa fa-chevron-left"></i> Stochastic Gradient Descent
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/duola.jpg"
                alt="lemelondeau" />
            
              <p class="site-author-name" itemprop="name">lemelondeau</p>
              <p class="site-description motion-element" itemprop="description">J'aime le melon d'eau.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lemelondeau" target="_blank" title="github">
                    
                      <i class="fa fa-fw fa-globe"></i>github</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Sample-data-from-a-distribution"><span class="nav-number">1.</span> <span class="nav-text">1. Sample data from a distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Monte-Carlo-sampling"><span class="nav-number">2.</span> <span class="nav-text">2. Monte Carlo sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Rejection-sampling"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Rejection sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Importance-sampling"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Importance sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Metropolis-Sampling"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Metropolis Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Markov-Chain-Monte-Carlo-MCMC"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 Markov Chain Monte Carlo (MCMC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-Markov-Chain"><span class="nav-number">2.5.</span> <span class="nav-text">2.4.1 Markov Chain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-Metropolis-Hastings-M-H-Algorithm"><span class="nav-number">2.6.</span> <span class="nav-text">2.4.2 Metropolis-Hastings (M-H) Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Approximate-Inference"><span class="nav-number">3.</span> <span class="nav-text">3. Approximate Inference</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number"></span> <span class="nav-text">References</span></a></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lemelondeau</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://lemelondeau.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://lemelondeau.github.io/2018/02/10/mcmc/';
          this.page.identifier = '2018/02/10/mcmc/';
          this.page.title = 'Monte Carlo Sampling';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://lemelondeau.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
