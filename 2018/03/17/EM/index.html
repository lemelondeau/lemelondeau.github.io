<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/duola.jpg?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/duola.jpg?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/duola.jpg?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="algorithms," />










<meta name="description" content="Notes of PRML and Machine Learning-A Probabilistic Prospective 1. Latent VariablesLatent variables, also called hidden variables or unobserved variables, are used in many probabolistic models to make">
<meta name="keywords" content="algorithms">
<meta property="og:type" content="article">
<meta property="og:title" content="The EM algorithm">
<meta property="og:url" content="https://lemelondeau.github.io/2018/03/17/EM/index.html">
<meta property="og:site_name" content="Cool Site">
<meta property="og:description" content="Notes of PRML and Machine Learning-A Probabilistic Prospective 1. Latent VariablesLatent variables, also called hidden variables or unobserved variables, are used in many probabolistic models to make">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-05-23T06:02:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The EM algorithm">
<meta name="twitter:description" content="Notes of PRML and Machine Learning-A Probabilistic Prospective 1. Latent VariablesLatent variables, also called hidden variables or unobserved variables, are used in many probabolistic models to make">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lemelondeau.github.io/2018/03/17/EM/"/>





  <title>The EM algorithm | Cool Site</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-80781234-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?//hm.baidu.com/hm.js?ee75cf111111aa99f8540efa2570970";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cool Site</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lemelondeau.github.io/2018/03/17/EM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lemelondeau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/duola.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cool Site">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">The EM algorithm</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-17T00:00:00+08:00">
                2018-03-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/17/EM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/17/EM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong><em>Notes of PRML and Machine Learning-A Probabilistic Prospective</em></strong></p>
<h2 id="1-Latent-Variables"><a href="#1-Latent-Variables" class="headerlink" title="1. Latent Variables"></a>1. Latent Variables</h2><p><em>Latent variables</em>, also called <em>hidden variables</em> or <em>unobserved variables</em>, are used in many probabolistic models to make it more tractable. Sometimes the observed variables are noisy, so there is a latent variable for each observed variable, such as in Gaussian process.</p>
<script type="math/tex; mode=display">
y=f+\epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>Where $y$ is the observed variable and $f$ is a latent variable.</p>
<p>Sometimes there is no noise but it is difficult to observe certain variables directly, whereas there are some observable variables related to them. The change of hidden variables will be reflected by the observable variables. For example, the variables in the hidden Markov model.</p>
<p>Latent variables can also be used to reduce the dimensionality of data, where the data points all lie close to a manifold of much lower dimensionality than that of the original data space [1].</p>
<p>Denote $X$ as observed variables and $Z$ as latent variables. A common used formula to ease the intractable problem using latent variable is</p>
<script type="math/tex; mode=display">
P(X)=\int_ZP(X,Z)</script><h2 id="2-The-Expectation-Maximization-Algorithm"><a href="#2-The-Expectation-Maximization-Algorithm" class="headerlink" title="2. The Expectation Maximization Algorithm"></a>2. The Expectation Maximization Algorithm</h2><h3 id="2-1-Gaussian-Mixture-Model"><a href="#2-1-Gaussian-Mixture-Model" class="headerlink" title="2.1 Gaussian Mixture Model"></a>2.1 Gaussian Mixture Model</h3><p>Gaussian distribution has some important analytical properties, but it suffers from signiﬁcant limitations when it comes to modelling real data sets [1]. For example, it is impossible to model a multimodal distribution. However, when we superpose several Gaussian distributions, the above mentioned limitation is overcome. The Gaussian mixture model (<strong>GMM</strong>) with $K$ components has the form</p>
<script type="math/tex; mode=display">
p(\textbf{x})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>Where $\sum \pi_k=1$ and $\mathbf{\mu}_k, \mathbf{\Sigma}_k$ are mean and variance for the $k^{\text{th}}$ component, and these are the parameters to be computed.</p>
<p><strong>The intractable derivative of likelihood function</strong></p>
<p>If we use the MLE method to estimate the parameters, the log likelihood function is given by</p>
<script type="math/tex; mode=display">
\log p(\mathbf{X}|\mathbf{\pi,\mu,\Sigma})=\sum_{n=1}^N\log \left\{\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}_n|\mathbf{\mu}_k, \mathbf{\Sigma}_k)\right\}</script><p>where $\mathbf{X}={\mathbf{x}_1,\mathbf{x}_2,…,\mathbf{x}_N}$.</p>
<p>Usually, if we want to maximize the log likelihood, we take the partial derivative over the parameters. Unfortunately, there is a summation of logarithm, making it impossible to find a closed-form analytical solution. One way to solve this problem is to apply gradient-based optimisation techniques [1]. An alternative approach is the expectation maximization (<strong>EM</strong>) algorithm.</p>
<h3 id="2-2-Gaussian-Mixture-Model-with-Latent-Variables"><a href="#2-2-Gaussian-Mixture-Model-with-Latent-Variables" class="headerlink" title="2.2 Gaussian Mixture Model with Latent Variables"></a>2.2 Gaussian Mixture Model with Latent Variables</h3><p>A $K$-dimensional binary random variable $\mathbf{z}$ is introduced. It has a $1$-of-$K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0, we can have</p>
<script type="math/tex; mode=display">
p(\mathbf{z})=\prod_{k=1}^K \pi_k^{z_k}</script><p>The joint distribution</p>
<script type="math/tex; mode=display">
p(\mathbf{x,z})=p(\mathbf{x|z})p(\mathbf z)</script><p>The marginal distribution of $\mathbf{x}$ is given by</p>
<script type="math/tex; mode=display">
p(\mathbf{x})=\sum_\mathbf{z}p(\mathbf z)p(\mathbf{x|z})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>We are now able to work with the joint distribution instead of the marginal distribution, and this will lead to significant simplification [1].</p>
<h3 id="2-3-The-EM-Algorithm"><a href="#2-3-The-EM-Algorithm" class="headerlink" title="2.3 The EM Algorithm"></a>2.3 The EM Algorithm</h3><p>Denote all parameters by $\pmb{\theta}$, and the log likelihood function is given by</p>
<script type="math/tex; mode=display">
\log p(\mathbf{X|\pmb\theta})=\log \left\{\sum_{\mathbf{Z} }p(\mathbf{X,Z|\pmb\theta})\right\}</script><p>Now consider the joint distribution for GMM,</p>
<script type="math/tex; mode=display">
p(\mathbf{X,Z|\pmb\theta})=\prod_{n=1}^N\prod_{k=1}^K\pi_k^{z_{nk}}\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)^{z_{nk}}</script><p>where $z_{nk}$ denotes the $k^{\text{th}}$ components of $\mathbf{z}_n$. Taking the logarithm, we obtain</p>
<script type="math/tex; mode=display">
\log p(\mathbf{X,Z|\pmb\theta})=\sum_{n=1}^N\sum_{k=1}^K z_{nk} \{\log\pi_k+\log\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)\}</script><p>The logarithm now acts directly on the Gaussian distribution. The  maximization with respect to a mean or a covariance is exactly for a single Gaussian.</p>
<p>The EM algorithm is given as follows:</p>
<p><strong>E step</strong>: find the expectation of the $\log p(\mathbf{X, Z}|\pmb\theta)$ over the posterior distribution of the latent variable</p>
<script type="math/tex; mode=display">
\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})=\sum_{\mathbf{Z}}p(\mathbf{Z|X, \pmb\theta}^{\text{old}})\log p(\mathbf{X,Z}|\pmb\theta)</script><p><strong>M step</strong>: maximize $\mathcal{Q}$</p>
<script type="math/tex; mode=display">
\pmb{\theta}^{\text{new}}={\arg\max}_{\pmb\theta}\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})</script><p><strong>What the hell does this mean? Where does the $\mathcal{Q}$ come from? Why does this equal to maximize $p(\mathbf{X|\pmb\theta})$? I felt I was eating a fly…</strong></p>
<p>We need to read further.</p>
<h2 id="3-EM-and-VI"><a href="#3-EM-and-VI" class="headerlink" title="3.EM and VI"></a>3.EM and VI</h2><h3 id="3-1-The-EM-Algorithm-in-General"><a href="#3-1-The-EM-Algorithm-in-General" class="headerlink" title="3.1 The EM Algorithm in General"></a>3.1 The EM Algorithm in General</h3><blockquote>
<p>The expectation maximization algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables.</p>
<p>Suppose that direct optimisation of $p(\mathbf{X}|\pmb{\theta})$ is difficult, but that optimisation of the complete-data likelihood function $p(\mathbf{X,Z|\pmb\theta})$ is significantly easier.</p>
</blockquote>
<p>To compute $p(\mathbf{X|\theta})=\int_{\mathbf Z} p(\mathbf{X,Z})$, the posterior distribution of $\mathbf Z$, $p(\mathbf{Z|X,\pmb\theta})$, is needed.</p>
<blockquote>
<p>Next introduce a distribution $q(\mathbf{Z})$ defined over the latent variable, which is supposed to be as close as possible to $p(\mathbf{Z|X,\pmb\theta})$.</p>
</blockquote>
<p>The following decomposition holds</p>
<script type="math/tex; mode=display">
\log p(\mathbf{X|\theta})=\mathcal{L}(q, \pmb{\theta})+\text{KL}(q||p)</script><p>where we have defined</p>
<script type="math/tex; mode=display">
\mathcal{L}(q,\pmb \theta)=\sum_{\mathbf Z}q(\mathbf Z)\log\left\{\frac{p(\mathbf{X,Z}|\pmb\theta)}{q(\mathbf{Z})}\right\} \\
\text{KL}(q||p)= - \sum_{\mathbf{Z} }\log \left\{\frac{p(\mathbf{Z|X,\pmb\theta})}{q(\mathbf Z)}\right\}</script><p>Since $\text{KL}(q||p) \geq 0$, then $\log p(\mathbf{X|\pmb\theta}) \geq \mathcal{L}(q, \pmb{\theta})$. $\mathcal L$ is a lower bound of the log likelihood.</p>
<p>Our aim is to maximize $\log p(\mathbf{X|\pmb\theta})$ hence $\mathcal L$ and there are two group of variables, $\mathbf Z$ and $\mathbf \theta$, to optimise.</p>
<p>Suppose the current value of the parameter vector is $\pmb \theta^\text {old}$</p>
<p><strong>E step:</strong> Fix $\pmb \theta^\text {old}$, maximize $\mathcal L$ w.r.t $q(\mathbf Z)$. <strong>Since $\pmb \theta^\text {old}$ is fixed,</strong> <strong>then $\log p(\mathbf{X|\pmb\theta})$ is fixed.</strong> The largest value of $\mathcal L$ will occur when $\text{KL}$ equals to zero. That means $q(\mathbf{Z})$ is equal to $p(\mathbf{Z|X},\pmb\theta^\text{old})$.</p>
<p><strong>M step:</strong> Fix $q(\mathbf Z) = p(\mathbf{Z|X},\pmb\theta^\text{old})$, maximize $\mathcal L$ w.r.t $\pmb \theta$. </p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal L(q,\pmb\theta)&=\sum_\mathbf{Z}p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{X,Z}|\pmb\theta)-\sum_\mathbf Z p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{Z|X},\pmb\theta^\text{old})\\
&=\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})+\text{const}
\end{align}</script><p><strong>Here comes the $\mathcal Q$ that is mentioned before.</strong></p>
<p>For an i.i.d data set,</p>
<script type="math/tex; mode=display">
p(\mathbf{Z|X},\pmb\theta)=\frac{p(\mathbf{Z},\mathbf{X}|\pmb\theta)}{\sum_\mathbf{Z} p(\mathbf{Z},\mathbf{X}|\pmb\theta)}=\frac{\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}{\sum_\mathbf{Z}\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}=\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)</script><h3 id="3-2-EM-and-VI"><a href="#3-2-EM-and-VI" class="headerlink" title="3.2 EM and VI"></a>3.2 EM and VI</h3><p>See <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">this article</a>.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
<p>​    [2]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p>
<p>​    [3]. <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/algorithms/" rel="tag"># algorithms</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/24/VI/" rel="next" title="Variational Inference">
                <i class="fa fa-chevron-left"></i> Variational Inference
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/01/EP/" rel="prev" title="Expectation Propagation">
                Expectation Propagation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/duola.jpg"
                alt="lemelondeau" />
            
              <p class="site-author-name" itemprop="name">lemelondeau</p>
              <p class="site-description motion-element" itemprop="description">J'aime le melon d'eau.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lemelondeau" target="_blank" title="github">
                    
                      <i class="fa fa-fw fa-globe"></i>github</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Latent-Variables"><span class="nav-number">1.</span> <span class="nav-text">1. Latent Variables</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-The-Expectation-Maximization-Algorithm"><span class="nav-number">2.</span> <span class="nav-text">2. The Expectation Maximization Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Gaussian-Mixture-Model"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Gaussian Mixture Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Gaussian-Mixture-Model-with-Latent-Variables"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Gaussian Mixture Model with Latent Variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-The-EM-Algorithm"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 The EM Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-EM-and-VI"><span class="nav-number">3.</span> <span class="nav-text">3.EM and VI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-The-EM-Algorithm-in-General"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 The EM Algorithm in General</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-EM-and-VI"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 EM and VI</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lemelondeau</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://lemelondeau.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://lemelondeau.github.io/2018/03/17/EM/';
          this.page.identifier = '2018/03/17/EM/';
          this.page.title = 'The EM algorithm';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://lemelondeau.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
