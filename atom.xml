<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cool Site</title>
  
  
  <link href="//atom.xml" rel="self"/>
  
  <link href="https://lemelondeau.github.io/"/>
  <updated>2018-01-28T09:29:24.000Z</updated>
  <id>https://lemelondeau.github.io/</id>
  
  <author>
    <name>lemelondeau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stochastic Gradient Descent</title>
    <link href="https://lemelondeau.github.io/2018/01/28/sgd/"/>
    <id>https://lemelondeau.github.io/2018/01/28/sgd/</id>
    <published>2018-01-28T08:49:36.000Z</published>
    <updated>2018-01-28T09:29:24.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><p>Gradient descent is one of the methods for optimisation, especially when the objective function is convex. </p><ul><li><p>Gradient</p><p>The first question is, what is <strong>gradient</strong>? In mathematics, the gradient is a multivariate generalization of the  derivative. </p><p>For a multivariate function $\varphi(x, y, z)$, the gradient can be written as the coordination form:<br>$$<br>\nabla \varphi(x, y, z)=\left(\frac{\partial\varphi}{\partial x},\frac{\partial\varphi}{\partial y},\frac{\partial\varphi}{\partial z}\right)<br>$$<br>The direction of $\nabla\varphi $ is the orientation in which the <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> has the largest value and $|\nabla\varphi|$ is the value of that <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> [1]. If we consider a function of two variables, we can get a tangent plane at any point on the surface. There will be many directions along this plane,  and the direction with the largest slope is the gradient at this point. That is to say, the gradient at a particular point is the direction that the function value changes the fastest.</p></li><li><p>Gradient descent algorithm</p><p>Gradient descent algorithm can be used to find the local minimum of a differentiable  function $\varphi(\mathbf{\mu})$. After knowing what the gradient is, it’ll be easy to understand the gradient descent algorithm. Starting from an initial point $\mathbf{x}_0$ (usually this point is important), walk a small step $\eta$ (the step size is also important) along the opposite direction of the gradient, it will reach a new point, then restart from the new point. Repeat this procedure until the different in output after taking a step is smaller than the threshold $\eta$.<br>There are three parameters in this algorithm, the initial point, the step size, and the threshold respectively. Consider the sequence $\mathbf{\mu}_0, \mathbf{\mu}_1, \mathbf{\mu}_2, …$, such that</p><p>$$\mathbf \mu_{t+1}=\mathbf \mu_t-\gamma\nabla\varphi(\mathbf \mu_t), t \geq 0$$</p></li><li><p>Gradient descent for linear regression</p><p>For linear regression, there are $n$ data points<br>$$<br>y_i=\mathbf x_i^T\mathbf w+b, i=1,2,…,n<br>$$</p><p>The objective function to minimize is<br>$$<br>J(\mathbf{w},b)=\frac{1}{n}\sum(y_i-\mathbf{x}_i^T\mathbf{w}-b)^2=\frac{1}{n}\sum J_i(\mathbf{w}, b)<br>$$</p><p>Now, we can use the gradient descent algorithm to compute $\mathbf w$ and $b$, denote them as \theta$:</p><p>$$<br>\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J(\mathbf \theta_t), t \geq 0<br>$$</p></li></ul><p>Code can be found <a href="https://github.com/lemelondeau/Algorithms/blob/master/SGD.ipynb" target="_blank" rel="external">here</a>.</p><h2 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h2><p>Many machine learning problems can be solved using gradient descent. However, when the dataset is very large, it will take a long time to update the  cost. Stochastic gradient descent (SGD) deals with this issue by updating the cost use only one sample each iteration:</p><p>$$<br>\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J_i(\mathbf \theta_t), t \geq 0<br>$$</p><p>The convergence of SGD is proven.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>​    [1]. <a href="http://mathworld.wolfram.com/Gradient.html" target="_blank" rel="external">http://mathworld.wolfram.com/Gradient.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Gradient-Descent&quot;&gt;&lt;a href=&quot;#1-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;1. Gradient Descent&quot;&gt;&lt;/a&gt;1. Gradient Descent&lt;/h2&gt;&lt;p&gt;Grad
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://lemelondeau.github.io/2017/10/14/hello-world/"/>
    <id>https://lemelondeau.github.io/2017/10/14/hello-world/</id>
    <published>2017-10-14T02:48:05.000Z</published>
    <updated>2017-10-14T07:12:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="test" scheme="https://lemelondeau.github.io/tags/test/"/>
    
  </entry>
  
</feed>
