<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cool Site</title>
  
  
  <link href="//atom.xml" rel="self"/>
  
  <link href="https://lemelondeau.github.io/"/>
  <updated>2018-05-23T06:08:05.000Z</updated>
  <id>https://lemelondeau.github.io/</id>
  
  <author>
    <name>lemelondeau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Bayesian Linear Regression and Gaussian Process</title>
    <link href="https://lemelondeau.github.io/2018/05/10/BLR/"/>
    <id>https://lemelondeau.github.io/2018/05/10/BLR/</id>
    <published>2018-05-10T11:48:29.000Z</published>
    <updated>2018-05-23T06:08:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML</em></strong></p><h2 id="1-Linear-Basis-Function-Models"><a href="#1-Linear-Basis-Function-Models" class="headerlink" title="1. Linear Basis Function Models"></a>1. Linear Basis Function Models</h2><h3 id="1-1-Model-Formalisation"><a href="#1-1-Model-Formalisation" class="headerlink" title="1.1 Model Formalisation"></a>1.1 Model Formalisation</h3><p>Linear regression is one of the most basic regression models for  supervised learning. It is a <strong>parametric model</strong>, that is to say, the number of parameters to be determined is fixed. During training, the objective of linear regression is to minimise the difference between real output and predicted output. Or equivalently, to maximise the likelihood function. </p><p>The simplest linear model for regression is</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+...+w_Dx_D=\mathbf{w}^T\mathbf{x}</script><p>where $\mathbf{x}=(x_1, … ,x_D)T$ and $\mathbf{w}=(w_0, w_1, … .w_D)$ which is the parameter. It is clear that the output $y$ is a linear combination of the input variables. The key property of this model is that <strong>it is a linear function of the parameters</strong> [1]. Even if the model have the form</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+w_1x_1^2+...+w_Dx_D^2 ,</script><p>it is still a linear regression model. This property encourages us to extend the model to be more flexible by considering linear combinations of fixed nonlinear functions of the input variables, with the following form</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+\sum _{j=1}^{M-1} w_j\phi_j(\mathbf{x})</script><p>where $\phi_j(\mathbf{x})$ are known as basis functions, $M$ is the number of parameters. Define $\phi_0(\mathbf x)=1$, we get</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=\sum_{j=0}^{M-1}w_j\phi_j{\mathbf x}=\mathbf w^T\pmb{\phi}(\mathbf x)</script><p>By using nonlinear basis functions, we allow the function $y(\mathbf{x},\mathbf{w})$ to be a non-linear function of the input vector $\mathbf x$.</p><h3 id="1-2-Model-Fitting"><a href="#1-2-Model-Fitting" class="headerlink" title="1.2 Model Fitting"></a>1.2 Model Fitting</h3><p>Given a dataset $\mathcal{D}(\mathbf{X,t})$, usually the observations are noisy. Assume the target variable $t$ is the noisy observation</p><script type="math/tex; mode=display">t=y(\mathbf{x},\mathbf{w})+\epsilon</script><p>where $\epsilon \sim \mathcal N(0, \beta^{-1})$. Thus we can write</p><script type="math/tex; mode=display">p(t|\mathbf x, \mathbf w, \beta)=\mathcal N(t|y(\mathbf{x},\mathbf{w}), \beta^{-1})</script><p><strong>Least Square Method</strong></p><p>The squared error is written as</p><script type="math/tex; mode=display">E_D(\mathbf w)= \frac{1}{2}\sum_{n=1}^{N}\{t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n)\}^2</script><p>Take the derivative respect to $\mathbf w$ and make it to be zero, we can get</p><script type="math/tex; mode=display">\mathbf w=(\Phi^T\Phi)^{-1}\Phi^T\mathbf t</script><p><strong>Maximum Likelihood</strong> </p><p>For i.i.d variables, the likelihood function of target variables is</p><script type="math/tex; mode=display">p(\mathbf t|\mathbf X, \mathbf w, \beta)=\prod_{n=1}^{N}\mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})</script><p>Taking the logarithm of the likelihood function </p><script type="math/tex; mode=display">\begin{align}\ln p(\mathbf t|\mathbf w, \beta)&=\sum_{n=1}^{N}\ln \mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})\\&=\sum_{n=1}^{N}\ln (\frac{\sqrt\beta}{\sqrt{2\pi}}\exp(-(t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n))^2\beta/2))\\&=\sum_{n=1}^{N}(\frac12\ln\beta-\frac12\ln(2\pi)-\frac12\beta\{t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n)\}^2)\\&=\frac N2\ln \beta-\frac N2\ln(2\pi)-\beta E_D(\mathbf w)\end{align}</script><p>Maximising the likelihood is therefore equivalent to minimising the square error.</p><h2 id="2-Bayesian-Linear-Regression"><a href="#2-Bayesian-Linear-Regression" class="headerlink" title="2. Bayesian Linear Regression"></a>2. Bayesian Linear Regression</h2><p>Over-fitting is a big issue when using maximum likelihood, Bayesian treatment is considered to avoid this.</p><h3 id="2-1-Posterior-Distribution-of-Parameter"><a href="#2-1-Posterior-Distribution-of-Parameter" class="headerlink" title="2.1 Posterior Distribution of Parameter"></a>2.1 Posterior Distribution of Parameter</h3><p>One of the aim of Bayesian linear regression is to get the distribution of parameter $\mathbf w$, which is written as $p(\mathbf w|\mathbf X, \mathbf t)$. This is a posterior distribution. Assume a Gaussian distribution over $\mathbf w$,</p><script type="math/tex; mode=display">p(\mathbf w)=\mathcal N(\mathbf w|\mathbf{m_0},\mathbf{S_0}).</script><p>The likelihood is also easy to get</p><script type="math/tex; mode=display">\begin{align}p(\mathbf t|\mathbf X, \mathbf w, \beta)&=\prod_{n=1}^{N}\mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})\\&=\mathcal N(\mathbf t|\Phi \mathbf w, \beta^{-1}I)\end{align}</script><p>Apply 2.116 conditional Gaussian we will get</p><script type="math/tex; mode=display">p(\mathbf w|\mathbf X, \mathbf t)=\mathcal N(\mathbf w|\mathbf{m}_N,\mathbf{S}_N).</script><p>where</p><script type="math/tex; mode=display">\mathbf m_N=\mathbf S_N(\mathbf S_0^{-1}\mathbf m_0+\beta\Phi^T\mathbf t)\\\mathbf S_N^{-1}=\mathbf S_0^{-1}+\beta\Phi^T\Phi</script><p>Now, assign a parameter $\alpha$ to control the prior distribution</p><script type="math/tex; mode=display">p(\mathbf w|\alpha)=\mathcal N(\mathbf w|\mathbf 0, \alpha^{-1}I)</script><p>and the corresponding posterior distribution is given by</p><script type="math/tex; mode=display">\mathbf m_N=\beta\mathbf S_N\Phi^T\mathbf t\\\mathbf S_N^{-1}=\alpha I+\beta\Phi^T\Phi</script><h3 id="2-2-Predictive-Distribution"><a href="#2-2-Predictive-Distribution" class="headerlink" title="2.2 Predictive Distribution"></a>2.2 Predictive Distribution</h3><p>To make prediction for new values of $\mathbf x$, we need to integrate $\mathbf w$ out. The predictive distribution is given by</p><script type="math/tex; mode=display">p(t|\mathbf x, \mathcal D,\alpha, \beta)=\int p(t|\mathbf x, \mathbf w, \beta)p(\mathbf w|\mathcal D,\alpha, \beta)\text{d}\mathbf w\\=\int \mathcal N(t|\mathbf w^T\phi (\mathbf x) ,\beta^{-1})\mathcal N(\mathbf w|\mathbf{m}_N,\mathbf{S}_N)\text{d}\mathbf w\\=\mathcal N(t|\mathbf m_N^T\phi(\mathbf x), \sigma_N^2(\mathbf x))</script><p>where $\sigma_N^2$ is given by (use 2.115 marginal distribution)</p><script type="math/tex; mode=display">\sigma_N^2(\mathbf x)=\frac1\beta+\phi(\mathbf x)^T\mathbf S_N\phi(\mathbf x)</script><h2 id="3-Relation-with-Gaussian-Process"><a href="#3-Relation-with-Gaussian-Process" class="headerlink" title="3. Relation with Gaussian Process"></a>3. Relation with Gaussian Process</h2><p>In the above Bayesian linear regression model, $\alpha$ and $\beta$ govern the predictive distribution. We call $\alpha$ and $\beta$ <strong>hyperparameters</strong>.</p><p>The hidden variables is given by</p><script type="math/tex; mode=display">y =y(\mathbf{x},\mathbf{w})=\sum_{j=0}^{M-1}w_j\phi_j{\mathbf x}=\mathbf w^T\pmb{\phi}(\mathbf x)</script><p>$\mathbf y$ is a linear combination of Gaussian distributed variables hence is itself Gaussian. The mean and covariance is given by</p><script type="math/tex; mode=display">\mathbb{E}[y] = \phi(\mathbf x)^T\mathbb{E}(\mathbf w)=\phi(\mathbf x)^T\mathbf{m}_N,\\\text{cov}[y,y']=\mathbb E [(y-\mathbb{E}[y])(y'-\mathbb{E}[y'])] =\phi(\mathbf x)^T\mathbf{S}_N\phi(\mathbf x^\prime),</script><p>We already have </p><script type="math/tex; mode=display">\mathbf m_N=\beta\mathbf S_N\Phi^T\mathbf t\\\mathbf S_N^{-1}=\alpha I+\beta\Phi^T\Phi</script><p>therefore,</p><script type="math/tex; mode=display">\mathbb E[y]=\beta\phi(\mathbf x)^T\mathbf S_N\Phi^T\mathbf t \\\text{var}[y]=\phi(\mathbf x)^T\mathbf S_N\phi(\mathbf x)</script><p>The mean and variance can be rewritten as following (though I didn’t try)</p><script type="math/tex; mode=display">\mathbb E[y]=\phi(\mathbf x)^T\Sigma_p\Phi(K+\beta^{-1}I)^{-1}\mathbf t\\\text{var}[y]=\phi(\mathbf x)^T\Sigma_p\phi(\mathbf x)-\phi(\mathbf x)^T\Sigma_p\Phi(K+\beta^{-1}I)^{-1}\Phi^T\Sigma_p\phi(\mathbf x)</script><p>where $\Sigma_p=\alpha^{-1} I$ and $K=\Phi^T \Sigma_p\Phi$.</p><p>Define $k(\mathbf x, \mathbf x^{\prime})=\phi(\mathbf x)^T\Sigma_p\phi(\mathbf x^{\prime})$, we can easily see Bayesian linear regression is equivalent to a Gaussian Process model, whose mean and variance of the predictive distribution are</p><script type="math/tex; mode=display">\pmb{\mu}=k(\mathbf x, \mathbf X)^T(K+\beta^{-1}I)^{-1}\mathbf t\\\Sigma=k(\mathbf x, \mathbf x)-k(\mathbf x, \mathbf X)^T (K+\beta^{-1}I)^{-1}k(\mathbf x, \mathbf X)</script><h2 id=""><a href="#" class="headerlink" title=""></a><!--4. The Evidence Approximation--></h2><!--marginalisation over hyperparameters--><!--laplace approximation--><!--EM--><!--log marginal likelihood--><p>(Finish writing on 2018-05-23 T^T)</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. Rasmussen, C. E., &amp; Williams, C. K. (2006). <em>Gaussian process for machine learning</em>. MIT press.</p><p>​    [3]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-Linear-Basis-Function-Models&quot;&gt;&lt;a href=&quot;#1-Linear-Basis-Function-Models&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Propagation</title>
    <link href="https://lemelondeau.github.io/2018/04/01/EP/"/>
    <id>https://lemelondeau.github.io/2018/04/01/EP/</id>
    <published>2018-04-01T06:07:08.000Z</published>
    <updated>2018-04-13T08:07:39.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML</em></strong></p><p>Expectation propagation (EP) is another deterministic approximate inference method besides variational inference (VI). The different between them is one minimises $KL(p||q)$ while another minimises $KL(q||p)$.</p><h2 id="1-KL-divergence"><a href="#1-KL-divergence" class="headerlink" title="1.KL divergence"></a>1.KL divergence</h2><script type="math/tex; mode=display">KL(p||q)=\int p(x)\log \frac{p(x)}{q(x)}\text{d}(x)</script><p>Where $p$ is the real distribution and $q$ is the approximation we want to find.  If we want to minimise $KL(p||q)$, we need to make sure when $p(x)$ is large, $q(x)$ also need to be large ($q(x)$ is non-zero when $p(x)$ is non-zero). On the other hand, if we want to minimise $KL(q||p)$, we need to make sure when $p(x)$ is close to zero, $q(x)$ is also near zero.</p><p><img src="https://github.com/lemelondeau/notes/blob/master/resources/KL.png" alt="KL divergence (right click to open)"></p><p>Figure 1. The blue contours show the real distribution $p$ and we want to approximate it with a Gaussian distribution. (a) The red contours represent a distribution minimises $KL(p||q)$. (b)(c) The red contours show distributions minimise $KL(q||p)$. [2]</p><h2 id="2-Sufficient-statistics-and-Exponential-family"><a href="#2-Sufficient-statistics-and-Exponential-family" class="headerlink" title="2. Sufficient statistics and Exponential family"></a>2. Sufficient statistics and Exponential family</h2><h3 id="2-1-Sufficient-statistics-1"><a href="#2-1-Sufficient-statistics-1" class="headerlink" title="2.1 Sufficient statistics [1]"></a>2.1 Sufficient statistics [1]</h3><p>Suppose we have a random sample $\textbf{x}_1,…,\textbf{x}_n$ taken from a distribution $f(\textbf{x}|\theta)$ which relies on an unknown parameter $\theta$ in a parameter space $\Theta$. The purpose of parameter estimation is to estimate the parameter $\theta$ from the random sample. Any function of the random sample $\textbf{x}_1,…,\textbf{x}_n$ is called a <strong>statistic</strong>. Mean and variance functions are both statistics.</p><p>If a statistic $T$ contains all the information to estimate $\theta$, then we say $T$ is <strong>sufficient</strong>. That is to say, given $T=t$, the random sample $\textbf{x}_1,…,\textbf{x}_n$ and parameter $\theta$ are independent of each other.</p><h5 id="Factorization-theorem"><a href="#Factorization-theorem" class="headerlink" title="Factorization theorem:"></a>Factorization theorem:</h5><p>A statistic $T(\textbf{x}_1,…,\textbf{x}_n)$ is a sufficient statistic for θ if and only if the joint pdf or the joint point mass function $f(\textbf{X}|\theta)$ of $\textbf{x}_1,…,\textbf{x}_n$ can be factorized as follows: </p><script type="math/tex; mode=display">f(\textbf{X}|\theta)=\mu(\textbf{X})\upsilon[T(\textbf{X}),\theta]</script><p>Here, the function $\mu$ and $\upsilon$ are nonnegative, the function $\mu$ may depend on $\textbf{x}$ but does not depend on $\theta$, and the function $\upsilon$ depends on $\theta$ but will depend on the observed value $\textbf{x}$ only through the value of the statistic $T(\textbf x)$.</p><h3 id="2-2-Exponential-family"><a href="#2-2-Exponential-family" class="headerlink" title="2.2 Exponential family"></a>2.2 Exponential family</h3><script type="math/tex; mode=display">p(\textbf x|\pmb\eta)=h(\textbf x)g(\pmb\eta)\exp\{\pmb \eta^T \textbf u(\textbf x)\}</script><script type="math/tex; mode=display">p(\textbf X|\pmb \eta)=\left(\prod_{n=1}^Nh(\textbf x_n)\right)g(\pmb \eta)^N\exp \left\{\pmb \eta^T \sum_{n=1}^N\textbf u(\textbf x)\right\}</script><p>According to the factorisation theorem, $\sum_{n=1}^N\textbf u(\textbf x)$ is the <em>sufficient statistic</em> of the distribution in exponential family. Therefore, $\mathbb E[\textbf u(\textbf x)]$ is also a sufficient statistic.</p><p>Taking the gradient of both sides of the following equation:</p><script type="math/tex; mode=display">g(\pmb \eta)\int h(\textbf x)\exp\{\pmb \eta^T \textbf u(\textbf x)\}\text d\textbf x=1</script><p>we will obtain</p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E[\textbf u(\textbf x)]</script><h2 id="3-Expectation-Propagation"><a href="#3-Expectation-Propagation" class="headerlink" title="3. Expectation Propagation"></a>3. Expectation Propagation</h2><p>Suppose $\mathcal D$ is observed data and $\textbf z$ is hidden variable (including parameters), we would like to know the posterior over $\textbf z$, $p(\textbf z|D)$. The aim of expectation propagation (EP) is to approximate a distribution $p(\textbf z)$ with another distribution $q(\textbf z)$ which is a member of the exponential family. The problem is to minimise $KL(p||q)$:</p><script type="math/tex; mode=display">KL(p||q)=-\ln g(\pmb \eta)-\pmb \eta^T\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]+\text{const}</script><p>We can minimise $ KL(p||q)$ by setting the gradient w.r.t. $\pmb \eta$ to zero and this results in</p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]</script><p>We have seen in section 2 that </p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E_{q(\textbf z)}[\textbf u(\textbf z)]</script><p>Therefore,</p><script type="math/tex; mode=display">\mathbb E_{q(\textbf z)}[\textbf u(\textbf z)]=\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]</script><p>That is to say, the optimum solution corresponds to matching the expected sufficient statistics. This is called <strong>moment matching</strong>.</p><p>Unfortunately, at this moment, averaging with respect to  the true distribution is intractable.</p><h5 id="A-product-of-factors"><a href="#A-product-of-factors" class="headerlink" title="A product of factors:"></a>A product of factors:</h5><p>Let’s look back at the posterior distribution itself.</p><script type="math/tex; mode=display">p(\textbf z|D)=\frac{p(D, \textbf z)}{p(D)}</script><p>where the joint distribution can be write as a product of factors in the form (given the data is i.i.d):</p><script type="math/tex; mode=display">p(D, \textbf z)=p(\textbf z)\prod_i f_i(\textbf z)</script><p>For each datapoint $\textbf x_i$, there is a factor $f_i(\textbf z)=p(\textbf x_i|\textbf z)$. $p(\textbf z)$ corresponds to the prior.</p><p>The approximation to the posterior distribution is also given by a product of factors</p><script type="math/tex; mode=display">q(\textbf z)=\frac 1Z\prod_i \widetilde{f}_i(\textbf z)</script><p>The KL divergence to minimise is now given by</p><script type="math/tex; mode=display">KL(p\|q)=KL\left(\frac{1}{p(D)}p(\textbf z)\prod_i f_i(\textbf z) \|\frac 1Z\prod_i \widetilde{f}_i(\textbf z)\right)</script><p>Instead of minimising the KL divergence w.r.t the true distribution, we can perform the minimisation between the corresponding pairs $f_i(\textbf z)$ and $\tilde f_i(\textbf z)$.</p><blockquote><p>Expectation Propagation</p><ol><li><p>Initialize all the approximating factors $\tilde f_i(\textbf z)$</p></li><li><p>Initialize the posterior approximation by setting</p><script type="math/tex; mode=display">q(\textbf z) \propto \prod_i \tilde f_i(\textbf z)</script></li><li><p>Until convergence:</p><p>a. Choose a factor $\tilde f_j(\textbf z)$ to refine.</p><p>b. Remove $\tilde f_j(\textbf z)$ from the posterior by division and get $q^{\setminus j}(\textbf z)$</p><p>c. Evaluate the new posterior by setting the sufficient statistics of $q^{\text{new}}(\textbf z)$ equal to those of $q^{\setminus j}(\textbf z)f_j(\textbf z)$, $(f_j(\textbf z)=p(\textbf x_j|\textbf z))$, evaluate the normalisation constant</p><script type="math/tex; mode=display">Z_j=\int q^{\setminus j}(\textbf z)f_j(\textbf z) \text d\textbf z</script><p>d. Evaluate and store the new factor</p><script type="math/tex; mode=display">\tilde f_j(\textbf z)=Z_j\frac{q^{\text{new}}(\textbf z)}{q^{\setminus j}(\textbf z)}</script></li></ol><!--belief propagation--> </blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. <a href="http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Sufficient.pdf" target="_blank" rel="external">http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Sufficient.pdf</a></p><p>​    [2]. Christopher M. Bishop. <em>Pattern Recognition and Machine Learning</em>. Springer, 2006.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Expectation propagation (EP) is another deterministic approximate inference method besides
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>The EM algorithm</title>
    <link href="https://lemelondeau.github.io/2018/03/17/EM/"/>
    <id>https://lemelondeau.github.io/2018/03/17/EM/</id>
    <published>2018-03-16T16:00:00.000Z</published>
    <updated>2018-05-23T06:02:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML and Machine Learning-A Probabilistic Prospective</em></strong></p><h2 id="1-Latent-Variables"><a href="#1-Latent-Variables" class="headerlink" title="1. Latent Variables"></a>1. Latent Variables</h2><p><em>Latent variables</em>, also called <em>hidden variables</em> or <em>unobserved variables</em>, are used in many probabolistic models to make it more tractable. Sometimes the observed variables are noisy, so there is a latent variable for each observed variable, such as in Gaussian process.</p><script type="math/tex; mode=display">y=f+\epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>Where $y$ is the observed variable and $f$ is a latent variable.</p><p>Sometimes there is no noise but it is difficult to observe certain variables directly, whereas there are some observable variables related to them. The change of hidden variables will be reflected by the observable variables. For example, the variables in the hidden Markov model.</p><p>Latent variables can also be used to reduce the dimensionality of data, where the data points all lie close to a manifold of much lower dimensionality than that of the original data space [1].</p><p>Denote $X$ as observed variables and $Z$ as latent variables. A common used formula to ease the intractable problem using latent variable is</p><script type="math/tex; mode=display">P(X)=\int_ZP(X,Z)</script><h2 id="2-The-Expectation-Maximization-Algorithm"><a href="#2-The-Expectation-Maximization-Algorithm" class="headerlink" title="2. The Expectation Maximization Algorithm"></a>2. The Expectation Maximization Algorithm</h2><h3 id="2-1-Gaussian-Mixture-Model"><a href="#2-1-Gaussian-Mixture-Model" class="headerlink" title="2.1 Gaussian Mixture Model"></a>2.1 Gaussian Mixture Model</h3><p>Gaussian distribution has some important analytical properties, but it suffers from signiﬁcant limitations when it comes to modelling real data sets [1]. For example, it is impossible to model a multimodal distribution. However, when we superpose several Gaussian distributions, the above mentioned limitation is overcome. The Gaussian mixture model (<strong>GMM</strong>) with $K$ components has the form</p><script type="math/tex; mode=display">p(\textbf{x})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>Where $\sum \pi_k=1$ and $\mathbf{\mu}_k, \mathbf{\Sigma}_k$ are mean and variance for the $k^{\text{th}}$ component, and these are the parameters to be computed.</p><p><strong>The intractable derivative of likelihood function</strong></p><p>If we use the MLE method to estimate the parameters, the log likelihood function is given by</p><script type="math/tex; mode=display">\log p(\mathbf{X}|\mathbf{\pi,\mu,\Sigma})=\sum_{n=1}^N\log \left\{\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}_n|\mathbf{\mu}_k, \mathbf{\Sigma}_k)\right\}</script><p>where $\mathbf{X}={\mathbf{x}_1,\mathbf{x}_2,…,\mathbf{x}_N}$.</p><p>Usually, if we want to maximize the log likelihood, we take the partial derivative over the parameters. Unfortunately, there is a summation of logarithm, making it impossible to find a closed-form analytical solution. One way to solve this problem is to apply gradient-based optimisation techniques [1]. An alternative approach is the expectation maximization (<strong>EM</strong>) algorithm.</p><h3 id="2-2-Gaussian-Mixture-Model-with-Latent-Variables"><a href="#2-2-Gaussian-Mixture-Model-with-Latent-Variables" class="headerlink" title="2.2 Gaussian Mixture Model with Latent Variables"></a>2.2 Gaussian Mixture Model with Latent Variables</h3><p>A $K$-dimensional binary random variable $\mathbf{z}$ is introduced. It has a $1$-of-$K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0, we can have</p><script type="math/tex; mode=display">p(\mathbf{z})=\prod_{k=1}^K \pi_k^{z_k}</script><p>The joint distribution</p><script type="math/tex; mode=display">p(\mathbf{x,z})=p(\mathbf{x|z})p(\mathbf z)</script><p>The marginal distribution of $\mathbf{x}$ is given by</p><script type="math/tex; mode=display">p(\mathbf{x})=\sum_\mathbf{z}p(\mathbf z)p(\mathbf{x|z})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>We are now able to work with the joint distribution instead of the marginal distribution, and this will lead to significant simplification [1].</p><h3 id="2-3-The-EM-Algorithm"><a href="#2-3-The-EM-Algorithm" class="headerlink" title="2.3 The EM Algorithm"></a>2.3 The EM Algorithm</h3><p>Denote all parameters by $\pmb{\theta}$, and the log likelihood function is given by</p><script type="math/tex; mode=display">\log p(\mathbf{X|\pmb\theta})=\log \left\{\sum_{\mathbf{Z} }p(\mathbf{X,Z|\pmb\theta})\right\}</script><p>Now consider the joint distribution for GMM,</p><script type="math/tex; mode=display">p(\mathbf{X,Z|\pmb\theta})=\prod_{n=1}^N\prod_{k=1}^K\pi_k^{z_{nk}}\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)^{z_{nk}}</script><p>where $z_{nk}$ denotes the $k^{\text{th}}$ components of $\mathbf{z}_n$. Taking the logarithm, we obtain</p><script type="math/tex; mode=display">\log p(\mathbf{X,Z|\pmb\theta})=\sum_{n=1}^N\sum_{k=1}^K z_{nk} \{\log\pi_k+\log\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)\}</script><p>The logarithm now acts directly on the Gaussian distribution. The  maximization with respect to a mean or a covariance is exactly for a single Gaussian.</p><p>The EM algorithm is given as follows:</p><p><strong>E step</strong>: find the expectation of the $\log p(\mathbf{X, Z}|\pmb\theta)$ over the posterior distribution of the latent variable</p><script type="math/tex; mode=display">\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})=\sum_{\mathbf{Z}}p(\mathbf{Z|X, \pmb\theta}^{\text{old}})\log p(\mathbf{X,Z}|\pmb\theta)</script><p><strong>M step</strong>: maximize $\mathcal{Q}$</p><script type="math/tex; mode=display">\pmb{\theta}^{\text{new}}={\arg\max}_{\pmb\theta}\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})</script><p><strong>What the hell does this mean? Where does the $\mathcal{Q}$ come from? Why does this equal to maximize $p(\mathbf{X|\pmb\theta})$? I felt I was eating a fly…</strong></p><p>We need to read further.</p><h2 id="3-EM-and-VI"><a href="#3-EM-and-VI" class="headerlink" title="3.EM and VI"></a>3.EM and VI</h2><h3 id="3-1-The-EM-Algorithm-in-General"><a href="#3-1-The-EM-Algorithm-in-General" class="headerlink" title="3.1 The EM Algorithm in General"></a>3.1 The EM Algorithm in General</h3><blockquote><p>The expectation maximization algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables.</p><p>Suppose that direct optimisation of $p(\mathbf{X}|\pmb{\theta})$ is difficult, but that optimisation of the complete-data likelihood function $p(\mathbf{X,Z|\pmb\theta})$ is significantly easier.</p></blockquote><p>To compute $p(\mathbf{X|\theta})=\int_{\mathbf Z} p(\mathbf{X,Z})$, the posterior distribution of $\mathbf Z$, $p(\mathbf{Z|X,\pmb\theta})$, is needed.</p><blockquote><p>Next introduce a distribution $q(\mathbf{Z})$ defined over the latent variable, which is supposed to be as close as possible to $p(\mathbf{Z|X,\pmb\theta})$.</p></blockquote><p>The following decomposition holds</p><script type="math/tex; mode=display">\log p(\mathbf{X|\theta})=\mathcal{L}(q, \pmb{\theta})+\text{KL}(q||p)</script><p>where we have defined</p><script type="math/tex; mode=display">\mathcal{L}(q,\pmb \theta)=\sum_{\mathbf Z}q(\mathbf Z)\log\left\{\frac{p(\mathbf{X,Z}|\pmb\theta)}{q(\mathbf{Z})}\right\} \\\text{KL}(q||p)= - \sum_{\mathbf{Z} }\log \left\{\frac{p(\mathbf{Z|X,\pmb\theta})}{q(\mathbf Z)}\right\}</script><p>Since $\text{KL}(q||p) \geq 0$, then $\log p(\mathbf{X|\pmb\theta}) \geq \mathcal{L}(q, \pmb{\theta})$. $\mathcal L$ is a lower bound of the log likelihood.</p><p>Our aim is to maximize $\log p(\mathbf{X|\pmb\theta})$ hence $\mathcal L$ and there are two group of variables, $\mathbf Z$ and $\mathbf \theta$, to optimise.</p><p>Suppose the current value of the parameter vector is $\pmb \theta^\text {old}$</p><p><strong>E step:</strong> Fix $\pmb \theta^\text {old}$, maximize $\mathcal L$ w.r.t $q(\mathbf Z)$. <strong>Since $\pmb \theta^\text {old}$ is fixed,</strong> <strong>then $\log p(\mathbf{X|\pmb\theta})$ is fixed.</strong> The largest value of $\mathcal L$ will occur when $\text{KL}$ equals to zero. That means $q(\mathbf{Z})$ is equal to $p(\mathbf{Z|X},\pmb\theta^\text{old})$.</p><p><strong>M step:</strong> Fix $q(\mathbf Z) = p(\mathbf{Z|X},\pmb\theta^\text{old})$, maximize $\mathcal L$ w.r.t $\pmb \theta$. </p><script type="math/tex; mode=display">\begin{align}\mathcal L(q,\pmb\theta)&=\sum_\mathbf{Z}p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{X,Z}|\pmb\theta)-\sum_\mathbf Z p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{Z|X},\pmb\theta^\text{old})\\&=\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})+\text{const}\end{align}</script><p><strong>Here comes the $\mathcal Q$ that is mentioned before.</strong></p><p>For an i.i.d data set,</p><script type="math/tex; mode=display">p(\mathbf{Z|X},\pmb\theta)=\frac{p(\mathbf{Z},\mathbf{X}|\pmb\theta)}{\sum_\mathbf{Z} p(\mathbf{Z},\mathbf{X}|\pmb\theta)}=\frac{\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}{\sum_\mathbf{Z}\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}=\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)</script><h3 id="3-2-EM-and-VI"><a href="#3-2-EM-and-VI" class="headerlink" title="3.2 EM and VI"></a>3.2 EM and VI</h3><p>See <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">this article</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p><p>​    [3]. <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML and Machine Learning-A Probabilistic Prospective&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-Latent-Variables&quot;&gt;&lt;a href=&quot;#1-Late
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="https://lemelondeau.github.io/2018/02/24/VI/"/>
    <id>https://lemelondeau.github.io/2018/02/24/VI/</id>
    <published>2018-02-24T03:48:47.000Z</published>
    <updated>2018-03-17T14:59:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Understanding-Variational-Inference"><a href="#1-Understanding-Variational-Inference" class="headerlink" title="1. Understanding Variational Inference"></a>1. Understanding Variational Inference</h2><p>I had a difficult time understanding what does the phrase <em>Variational Inference</em> mean. First, let’s see what is <em>inference</em>. It refers to <em>statistical inference</em> which means to estimate or <strong>infer</strong> the parameters/hidden variables of a statistical model. For example, estimating the weights in linear regression. In general, there are two different statistical inference methods, namely <em>Frequentist method</em> and <em>Bayesian inference</em>. Still use the linear regression as an example, frequentist method will give a estimated value for each weight while the Bayesian inference will produce a distribution over the weights. What is going to be used in variational inference is Bayesian inference, where posterior distribution is derived by the Bayes’ rule.</p><p>What does <em>variational</em> refer to? Variation is a concept in <em>functional</em> ( mappings from a set of functions to the real numbers. It’s to find a function which minimise or maximise the functional. If we want to get the optima of a function, usually we can take the derivative, which is called differential. Similarly, if we take the derivative of a functional, it is called variational.</p><p>As we know, posterior distribution is notoriously intractable. In variational inference, we aim to find an approximation to it. During this process, a functional is maximised, where the input of the optima is the approximation mentioned just now.</p><h2 id="2-Tackle-Posterior-with-Variational-Inference"><a href="#2-Tackle-Posterior-with-Variational-Inference" class="headerlink" title="2. Tackle Posterior with Variational Inference"></a>2. Tackle Posterior with Variational Inference</h2><h3 id="2-1-Problem-Formalisation"><a href="#2-1-Problem-Formalisation" class="headerlink" title="2.1 Problem Formalisation"></a>2.1 Problem Formalisation</h3><p>Suppose we have observations $X$ and latent random variables $Z$. Here, the latent random variables include the parameters of model and the latent variables whose counterparts are the observations. We are going to compute $P(Z|X)$, the posterior which is</p><script type="math/tex; mode=display">p(\mathbf{Z}|\mathbf{X})=\frac{p(\mathbf{X}|\mathbf{Z})p(\mathbf{Z})}{\int p(\mathbf{X},\mathbf{Z})p(\mathbf{Z}) d\mathbf Z}</script><p>However, for many models, the denominator may not have a closed-form solution and the latent variable might be high dimensional. Therefore, we want to find an approximation $q(\mathbf{Z})$ of $p(\mathbf{Z}|\mathbf{X})$, such that $KL(q||p)$ is minimised. We assume $q(Z)$ a Gaussian distribution.</p><script type="math/tex; mode=display">\begin{align}KL(q||p) &=KL(q(\mathbf{Z})||p(\mathbf Z|\mathbf X)) \\ &=\int q(\mathbf Z)\log \frac{q(\mathbf Z)}{p(\mathbf Z|\mathbf X)} d\mathbf Z\\&=\int q(\mathbf Z) \log \frac{q(\mathbf Z)p(\mathbf X)}{p(\mathbf {X},\mathbf{Z})}d \mathbf Z\\&=\int q(\mathbf Z)[\log q(\mathbf Z)+\log p(\mathbf X)-\log p(\mathbf  X, \mathbf  Z)] d \mathbf Z\\&=\int q(\mathbf  Z)[\log q(\mathbf Z)-\log p(\mathbf X,\mathbf Z)]d \mathbf Z+\log p(\mathbf X)\end{align}  \qquad(1)</script><h3 id="2-2-The-evidence-lower-bound-ELBO"><a href="#2-2-The-evidence-lower-bound-ELBO" class="headerlink" title="2.2 The evidence lower bound(ELBO)"></a>2.2 The evidence lower bound(ELBO)</h3><p>Rearrange the above equation (1), we get</p><script type="math/tex; mode=display">\log p(\mathbf X)=KL(q||p)-\int q(\mathbf  Z)[\log q(\mathbf Z)-\log p(\mathbf X,\mathbf Z)]d \mathbf Z</script><p>That is </p><script type="math/tex; mode=display">\log p(\mathbf X)=KL(q||p)+E_{q(\mathbf Z)}[\log p(\mathbf X, \mathbf Z)-\log q(\mathbf Z)]</script><p>Since $KL(q||p)\geq 0$ and $\log p(\mathbf X)$ is a constant, then minimizing $KL(q||p)$ is equivalent to maximizing the second term. Also, the second term is called the <em>evidence lower bound (ELBO)</em>:</p><script type="math/tex; mode=display">\mathcal L(p)=\int q(\mathbf Z)\log \frac {p(\mathbf X, \mathbf Z)}{q(\mathbf Z)} d \mathbf Z \qquad\qquad\qquad(2)</script><p>This is the <strong>functional</strong> we want to maximise.</p><blockquote><p>The first term is an expected likelihood; it encourages densities that place their mass on configurations of the latent variables that explain the observed data. The second term is the negative divergence between the variational density and the prior; it encourages densities close to the prior. Thus the variational objective mirrors the usual balance between likelihood and prior. [5]</p></blockquote><p>The lower bound can also be obtained by applying Jensen’s inequality:</p><script type="math/tex; mode=display">\begin{align}\log p(\mathbf{X})&=\log\int p(\mathbf X, \mathbf Z) d \mathbf{Z}\\&=\log \int p(\mathbf X,\mathbf Z)\frac{q(\mathbf Z)}{q(\mathbf Z)} d \mathbf Z\\&=\log (E_q\left[ \frac{p(\mathbf X,\mathbf Z)}{q(\mathbf Z)}\right])\\&\geq E_q\left[\log p(\mathbf X,\mathbf Z)-q(\mathbf Z)\right]\\&=\int q(\mathbf Z)\log \frac {p(\mathbf X, \mathbf Z)}{q(\mathbf Z)} d \mathbf Z\end{align}</script><h3 id="2-3-Maximising-the-ELBO-and-Get-the-Approximation"><a href="#2-3-Maximising-the-ELBO-and-Get-the-Approximation" class="headerlink" title="2.3 Maximising the ELBO and Get the Approximation"></a>2.3 Maximising the ELBO and Get the Approximation</h3><p>We consider a restricted family of distributions $q(\mathbf Z)$ and then seek the member of this family for which the KL divergence is minimized. Our goal is to restrict the family sufﬁciently that they comprise only tractable distributions, while at the same time allowing the family to be sufﬁciently rich and ﬂexible that it can provide a good approximation to the true posterior distribution [1].</p><h4 id="2-3-1-Mean-field-variational-inference"><a href="#2-3-1-Mean-field-variational-inference" class="headerlink" title="2.3.1 Mean field variational inference"></a>2.3.1 Mean field variational inference</h4><p>Partition the elements of $\mathbf Z$ into disjoint groups $\mathbf Z_i$ where $i=1,2,…M$. Then the factorization of $q$ is</p><script type="math/tex; mode=display">q(\mathbf Z)=\prod_{i=1}^M q_i(\mathbf Z_i)</script><p>Then the ELBO can be refactorized as</p><script type="math/tex; mode=display">\begin{align}\mathcal L&=\int q(\mathbf Z) \log p(\mathbf X, \mathbf Z)d \mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\int q(\mathbf Z) \log[p(\mathbf X)\prod_i p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\log p(\mathbf X)+\int q(\mathbf Z) \log[\prod_ip(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\log p(\mathbf X)+\int q(\mathbf Z) \sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\\end{align} \qquad(3)</script><p>Consider the partitions $\mathbf Z={\mathbf Z_j, \overline{\mathbf Zj}}$ where $ \overline{\mathbf Zj}=\mathbf Z\setminus \mathbf Z_j $.</p><script type="math/tex; mode=display">\begin{align}\int q(\mathbf Z)\sum_i \log q_i d\mathbf Z&=\int \prod_i q_i\sum_i\log q_i d\mathbf Z\\&=\sum_i \int \left( \prod_i q_i\right)\log q_i d\mathbf Z\\&=\sum_i \int \int q_j \overline{q_j} \log q_i d\mathbf Z_j d\mathbf {\overline{\mathbf Z}}_ j\end{align}</script><p>Let $i=j$, then the above formula</p><script type="math/tex; mode=display">\begin{align}&=\sum_i \int \int q_i \overline{q_i} \log q_i d\mathbf Z_i d\mathbf {\overline{\mathbf Z}}_i\\&=\sum_i \int \log q_i d\mathbf Z_i \\\end{align}</script><p>Similarly,</p><script type="math/tex; mode=display">\begin{align}\int q(\mathbf Z) \sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z&=\int \int q_i \overline{q_i}\sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z_id\overline{\mathbf Z_i}\\&=\sum_i \int \int q_i \overline{q_i}\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z_id\overline{\mathbf Z_i}\\&=\sum_i \int \left(\int \overline{q_i}\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\overline{\mathbf Z_i}\right) q_i d\mathbf Z_i\\&=\sum_i E_i\left[E_{-i}[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]\right]\\&=\sum_i E[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]\end{align}</script><p>So we can get</p><script type="math/tex; mode=display">\mathcal L=\log p(\mathbf X)+\sum_i E[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]-\sum_i \int \log q_i d\mathbf Z_i \\</script><h4 id="2-3-2-Coordinate-Ascent"><a href="#2-3-2-Coordinate-Ascent" class="headerlink" title="2.3.2 Coordinate Ascent"></a>2.3.2 Coordinate Ascent</h4><p>Now we fix all the elements in $\mathbf Z$ except for $\mathbf Z_j$ whose is the last variable in the above chain rule list.</p><p>Write the objective as a function of $q_j=q(\mathbf Z_j)$</p><script type="math/tex; mode=display">\mathcal L_j=\int q_j E_{-j}[\log p(\mathbf Z_i|\mathbf Z_{-j},\mathbf X)]d\mathbf Z_j-\int q_j\log q_j d\mathbf Z_j+ \text {const}</script><p>Take the derivative of $\mathcal L_j$ (<strong>variational</strong>),</p><script type="math/tex; mode=display">\frac{\mathcal L_j}{d q_j}=E_{-j}[\log p(\mathbf Z_i|\mathbf Z_{-j},\mathbf X)]- \log q_j-1=0</script><script type="math/tex; mode=display">q_j^* \propto \exp\left\{E_{-j}[\log p(\mathbf Z_j|\mathbf Z_{-j}, \mathbf X)]\right\}</script><p>the denominator of the posterior does not depend the $\mathbf Z_j$, so</p><script type="math/tex; mode=display">q_j^* \propto \exp\left\{E_{-j}[\log p(\mathbf Z_j,\mathbf Z_{-j}, \mathbf X)]\right\}</script><p>Update $q_j$ iteratively until the convergence of ELBO.</p><h2 id="3-Variational-Inference-vs-MCMC"><a href="#3-Variational-Inference-vs-MCMC" class="headerlink" title="3. Variational Inference vs MCMC"></a>3. Variational Inference vs MCMC</h2><blockquote><p>MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density (Robert and Casella, 2004). Variational inference does not enjoy such guarantees—it can only find a density close to the target—but tends to be faster than MCMC. Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. [5]</p></blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. springer.</p><p>[2] <a href="https://benmoran.wordpress.com/2015/02/21/variational-bayes-and-the-evidence-lower-bound/" target="_blank" rel="external">https://benmoran.wordpress.com/2015/02/21/variational-bayes-and-the-evidence-lower-bound/</a></p><p>[3] <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf" target="_blank" rel="external">https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf</a></p><p>[4] Fox, C. W., &amp; Roberts, S. J. (2012). A tutorial on variational Bayesian inference. <em>Artificial intelligence review</em>, 1-11.</p><p>[5] Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017). Variational inference: A review for statisticians. <em>Journal of the American Statistical Association</em>, <em>112</em>(518), 859-877.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Understanding-Variational-Inference&quot;&gt;&lt;a href=&quot;#1-Understanding-Variational-Inference&quot; class=&quot;headerlink&quot; title=&quot;1. Understanding V
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Monte Carlo Sampling</title>
    <link href="https://lemelondeau.github.io/2018/02/10/mcmc/"/>
    <id>https://lemelondeau.github.io/2018/02/10/mcmc/</id>
    <published>2018-02-10T06:44:53.000Z</published>
    <updated>2018-02-10T15:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Sample-data-from-a-distribution"><a href="#1-Sample-data-from-a-distribution" class="headerlink" title="1. Sample data from a distribution"></a>1. Sample data from a distribution</h2><p>Sometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among all the distributions, the easiest one to sample from is uniform distribution $U(0,1)$. One thing worth noticing is most of the random generators generate pseudo random numbers rather than genuinely ones. Two commonly used methods are <strong><em>linear congruential generator</em></strong> (LCG) and <strong><em>multiplicative congruential generator</em></strong> (MCG):</p><script type="math/tex; mode=display">\text{LCG:}\left\{      \begin{array}{ll}      x_i=(C+\lambda x_{i-1} ) \text{mod}  M \\      r_i=x_i/M      \end{array}\right.</script><script type="math/tex; mode=display">\text{MCG:}\left\{      \begin{array}{ll}      x_i=\lambda x_{i-1} \text{mod} M\\      r_i=x_i/M      \end{array}\right.</script><p>where $x_0$ is called the seed, $C$ is a non-negative integer and $r_i$ is the generated random number. For further information, please refer to <a href="http://statweb.stanford.edu/~owen/mc/Ch-unifrng.pdf" target="_blank" rel="external">this</a>.</p><p>Now we have the random numbers from $U(0,1)$, how about other distributions? One well known method is <strong><em>inversion sampling</em></strong>.</p><p>Given a distribution $P(X)$, here $X$ is a continuous variable, we know that the output range of its cumulative distribution function $C(X)$ is $[0, 1]$. If we sample a random number $u$ from $U(0, 1)$, there will be always an $x$ such that $C(x)=u$. Now, $x$ is a sample from $P(X)$. To compute $x$ from $u$, we need the inverse function $C^{-1}$.</p><script type="math/tex; mode=display">x=C^{-1}(u)</script><p>What if $F(X)​$ is not invertible or we don’t even have the formula of $P(X)​$? </p><h2 id="2-Monte-Carlo-sampling"><a href="#2-Monte-Carlo-sampling" class="headerlink" title="2. Monte Carlo sampling"></a>2. Monte Carlo sampling</h2><p>In this part, we assume $p(x)$ is known.</p><h3 id="2-1-Rejection-sampling"><a href="#2-1-Rejection-sampling" class="headerlink" title="2.1 Rejection sampling"></a>2.1 Rejection sampling</h3><p>We want to sample data from a distribution $p(x)$, however, it is very difficult. Suppose we are lucky to have an easy-to-sample distribution $q(x)$, such that $kq(x) \geq p(x), k&gt;0$. $q(x)$ is called proposal distribution. What we are going to do is first sample a data $x_0$ from $q(x)$, and then sample $u_0$ from $U(0, kq(x_0))$. If $u_0&gt;p(x_0)$, the sample is rejected. This method is useful in one or two dimensions. [1]</p><h3 id="2-2-Importance-sampling"><a href="#2-2-Importance-sampling" class="headerlink" title="2.2 Importance sampling"></a>2.2 Importance sampling</h3><p>Suppose we have an easy-to-sample proposal distribution $q(x)$, such that $q(x)&gt;0$ if $p(x)&gt;0$. Now, we want to get the expectation of $f(x)$.</p><script type="math/tex; mode=display">\begin{align}E[f]&=\int f(x)p(x)\text{d}x\\&=\int f(x)\frac{p(x)}{q(x)}q(x)\text{d}x\\&\approx \frac1N\sum_n\frac{p(x_n)}{q(x_n)}q(x_n), x_n \sim q(x)\end{align}</script><p>$w_n=\frac{p(x_n)}{q(x_n)}$ is called <strong>importance weight</strong>. The above formula means sampling $f(x)$ from $p(x)$ is equivalent to sampling $f(x)w(x)$ from $q(x)$. All samples are retained, which is different from the reject sampling.</p><h3 id="2-3-Metropolis-Sampling"><a href="#2-3-Metropolis-Sampling" class="headerlink" title="2.3 Metropolis Sampling"></a>2.3 Metropolis Sampling</h3><p>The idea of Metropolis sampling is quite simple: from an initial position $x_0$, apply random walks to propose the next position and decide whether to accept it. If the new position $x_{t+1}$ is more likely to be visited than $x_t$ under $p(x)$, that is $p(x_{t+1})\geq p(x_t)$, the proposed position is accepted. Otherwise, accept the proposed position with probability $p(x_{t+1})/p(x_t)$, set $x_{t+1}$ to $x_t$. For the random walks, a Gaussian distribution is usually used: $p(x_{t+1}|x_t)\sim \mathcal{N}(x_t, 1)$.</p><p>I really hate some of the posts given the M-H algorithm (see section 2.4) first and say Metropolis sampling is a special case of it. Especially when they give the acceptance rate without further explanation, which makes me feel like I am the most stupid person in the world. </p><p>FYI: The Metropolis algorithm was first proposed in 1953. It was then generalized by Hastings in 1970. [2]</p><h3 id="2-4-Markov-Chain-Monte-Carlo-MCMC"><a href="#2-4-Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="2.4 Markov Chain Monte Carlo (MCMC)"></a>2.4 Markov Chain Monte Carlo (MCMC)</h3><p>What is not so good with Metropolis sampling? What is the intuition of MCMC?</p><h3 id="2-4-1-Markov-Chain"><a href="#2-4-1-Markov-Chain" class="headerlink" title="2.4.1 Markov Chain"></a>2.4.1 Markov Chain</h3><p>For Markov Chain, there are $n$ states $x_1, x_2, … ,x_n$ and the state at time $t+1$ is conditional independent to other historical states given the state at time $t$</p><script type="math/tex; mode=display">p(x_{t+1}|x_1, x_2,...,x_t)=p(x_{t+1}|x_t)</script><p>$p$ is called transition probability. And the transition function $T$:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t) \equiv p(x_{t+1}|x_t)</script><p>Starting form a certain state, we can get the distribution $\pi_t(x)$ at any time $t$ using the transition function. We call $\pi(x)$ a <strong>stationary distribution</strong> when it doesn’t change any more:</p><script type="math/tex; mode=display">\pi(x)=\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)</script><p>To make $\pi(x)$ stationary, a sufficient but not necessary condition is</p><script type="math/tex; mode=display">\pi(x^\prime)T(x\leftarrow x^\prime)=\pi(x)T(x^\prime \leftarrow x)</script><p>The above property is called <strong>detailed balance</strong> and we can prove the stationarity with this property:</p><script type="math/tex; mode=display">\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)=\sum_{x^\prime}\pi(x)T(x^\prime\leftarrow x)=\pi(x)</script><h3 id="2-4-2-Metropolis-Hastings-M-H-Algorithm"><a href="#2-4-2-Metropolis-Hastings-M-H-Algorithm" class="headerlink" title="2.4.2 Metropolis-Hastings (M-H) Algorithm"></a>2.4.2 Metropolis-Hastings (M-H) Algorithm</h3><p>M-H algorithm is one of the classical MCMC methods. In this section, we will see how to apply Markov Chain, especially it’s stationary property, to do sampling.</p><p><strong>A Markov chain view of Metropolis sampling</strong></p><p>Let’s say now we want to sampling from $\pi(x)$, we know that when it is stationary the next position $x_{t+1}$ we get by applying the transition function always has distribution $\pi(x)$. That means we can take $x_{t+1}$ as a sample of $\pi(x)$ ! Wow!</p><p>Now our problem is that we don’t know the exact $T$. What can we do?</p><p>Let’s look back at the Metropolis method in a Markov chain way. At position $x_t$, we want to know where will the next position be. In Markov chain, we get $x_{t+1}$ using $T(x_{t+1}\leftarrow x_t)$. In Metropolis method, there are two steps, first propose a new position using $Q(x_{t+1}\leftarrow x_t)$ and then decide whether to accept it using $A(x_{t+1}\leftarrow x_t)$. It’s now obvious that</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)</script><p>And if $x_{t+1}=x_t$, there are two parts, one if the proposal is itself and another is when the proposal is rejected:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)+\sum_{x^\prime}Q(x^\prime\leftarrow x_t)(1-A(x^\prime\leftarrow x_t))</script><p>We can specify the proposal distribution, what we don’t have is the acceptance rate $A(x_{t+1}\leftarrow x_t)$. How can we get it?</p><p><strong>Acceptance rate</strong></p><p>We hope that the chosen acceptance rate will give us a stationary distribution such that the states produced by the Markov chain are from the distribution $\pi(x)$. Applying the <strong>detailed balance</strong> we will get</p><script type="math/tex; mode=display">\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)=\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)</script><p>Subsequently, we can get</p><script type="math/tex; mode=display">%\frac{A(x^\prime \leftarrow x)}{A(x\leftarrow x^\prime)}=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}\\A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}A(x\leftarrow x^\prime)</script><p>Ideally, we want the acceptance rate to be <strong>as high as possible</strong>, so we choose $A(x\leftarrow x^\prime)$ to be 1 and $A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}$. But at the meanwhile, it is not larger than 1. So the final result would be</p><script type="math/tex; mode=display">A(x^\prime \leftarrow x)=\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)</script><p>We can prove the stationarity:</p><script type="math/tex; mode=display">\begin{align}\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)&=\pi(x)Q(x^\prime\leftarrow x)\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)\\&=\min(\pi(x^\prime)Q(x\leftarrow x^\prime), \pi(x)Q(x^\prime\leftarrow x)) \\&=\pi(x^\prime)Q(x\leftarrow x^\prime)\min(1, \frac{\pi(x)Q(x^\prime \leftarrow x)}{\pi(x^\prime)Q(x\leftarrow x^\prime)})\\&=\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)\end{align}</script><p>OK, now we can see why it is said Metropolis sampling is a special case of M-H algorithm. When $Q(x^\prime \leftarrow x)=Q(x \leftarrow x^\prime)$, M-H algorithm becomes Metropolis sampling.</p><p><strong>The algorithm</strong></p><ol><li>start from $x_0$</li><li>propose candidate $x^\prime$ position according to some proposal distribution $p(x^\prime|x)$</li><li>accept the candidate with probability $\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)$</li><li>if accepted, set $x_{t+1}=x^\prime$, otherwise $x_{t+1}=x_{t}$</li></ol><p><strong>Burn-in</strong></p><p>If the starting position is far from the dense area, it will take some time for the algorithm to reach the dense area (more representative of the distribution). Therefore, burn-in is needed.</p><p>Two good videos are <a href="https://www.youtube.com/watch?v=sK3cg15g8FI&amp;list=LLT7nvYvSbuz8tuy5ZqcCVYQ" target="_blank" rel="external">Machine learning - Markov chain Monte Carlo (MCMC) II</a> and<a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/hnzut/metropolis-hastings" target="_blank" rel="external">Metropolis-Hastings</a>.</p><h2 id="3-Approximate-Inference"><a href="#3-Approximate-Inference" class="headerlink" title="3. Approximate Inference"></a>3. Approximate Inference</h2><p>In practice, we usually want to sample from a distribution whose formula is unknown. The most common one is posterior $p(\theta|D)$.</p><script type="math/tex; mode=display">p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}</script><p>Usually, $p(D)=\int p(D|\theta)p(\theta)\text{d}\theta $ is intractable, how are we going to deal with this?</p><p>Notice that $p(D)$ is a constant and we are taking division so it doesn’t matter what it would be! This makes MCMC a powerful method for approximating posterior distributions.</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>​    [1]. <a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf" target="_blank" rel="external">http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf</a></p><p>​    [2]. <a href="https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\" target="_blank" rel="external">https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\</a></p><p>​    [3]. <a href="http://www.maths.nuigalway.ie/~dane/Friel.pdf" target="_blank" rel="external">http://www.maths.nuigalway.ie/~dane/Friel.pdf</a></p><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Sample-data-from-a-distribution&quot;&gt;&lt;a href=&quot;#1-Sample-data-from-a-distribution&quot; class=&quot;headerlink&quot; title=&quot;1. Sample data from a dist
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Gradient Descent</title>
    <link href="https://lemelondeau.github.io/2018/01/28/SGD/"/>
    <id>https://lemelondeau.github.io/2018/01/28/SGD/</id>
    <published>2018-01-28T08:49:36.000Z</published>
    <updated>2018-02-10T15:09:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><p>Gradient descent is one of the methods for optimisation, especially when the objective function is convex. </p><ul><li><p>Gradient</p><p>The first question is, what is <strong>gradient</strong>? In mathematics, the gradient is a multivariate generalization of the derivative. </p><p>For a multivariate function $\varphi(x, y, z)$, the gradient can be written as the coordination form:</p><script type="math/tex; mode=display">\nabla \varphi(x, y, z)=\left(\frac{\partial\varphi}{\partial x},\frac{\partial\varphi}{\partial y},\frac{\partial\varphi}{\partial z}\right)</script><p>The direction of $\nabla\varphi $ is the orientation in which the <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> has the largest value and $|\nabla\varphi|$ is the value of that <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> [1]. If we consider a function of two variables, we can get a tangent plane at any point on the surface. There will be many directions along this plane,  and the direction with the largest slope is the gradient at this point. That is to say, the gradient at a particular point is the direction that the function value changes the fastest.</p></li><li><p>Gradient descent algorithm</p><p>Gradient descent algorithm can be used to find the local minimum of a differentiable  function $\varphi(\mathbf{\mu})$. After knowing what the gradient is, it’ll be easy to understand the gradient descent algorithm. Starting from an initial point $\mathbf{x}_0$ (usually this point is important), walk a small step $\eta$ (the step size is also important) along the opposite direction of the gradient, it will reach a new point, then restart from the new point. Repeat this procedure until the different in output after taking a step is smaller than the threshold $\eta$.<br>There are three parameters in this algorithm, the initial point, the step size, and the threshold respectively. Consider the sequence $\mathbf{\mu}_0, \mathbf{\mu}_1, \mathbf{\mu}_2, …$, such that</p><script type="math/tex; mode=display">\mathbf \mu_{t+1}=\mathbf \mu_t-\gamma\nabla\varphi(\mathbf \mu_t), t \geq 0</script></li><li><p>Gradient descent for linear regression</p><p>For linear regression, there are $n$ data points</p><script type="math/tex; mode=display">y_i=\mathbf x_i^T\mathbf w+b, i=1,2,...,n</script><p>The objective function to minimize is</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{n}\sum_{i=1}^{n}J_i(\theta)= \frac{1}{n}\sum_{i=1}^n J_i(\mathbf{w}, b)</script></li></ul><p>  Now, we can use the gradient descent algorithm to compute $\mathbf w$ and $b$, denote them as \theta$:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J(\mathbf \theta_t), t \geq 0</script><p>Code can be found <a href="https://github.com/lemelondeau/Algorithms/blob/master/SGD.ipynb" target="_blank" rel="external">here</a>.</p><h2 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h2><p>Many machine learning problems can be solved using gradient descent. However, when the dataset is very large, it will take a long time to update the  cost. Stochastic gradient descent (SGD) deals with this issue by updating the cost use only one sample each iteration:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J_i(\mathbf \theta_t), t \geq 0</script><p>Gradient descent:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \frac{1}{n}\sum_{i=1}^{n}\nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>SGD:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>The convergence of SGD is proven.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. <a href="http://mathworld.wolfram.com/Gradient.html" target="_blank" rel="external">http://mathworld.wolfram.com/Gradient.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Gradient-Descent&quot;&gt;&lt;a href=&quot;#1-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;1. Gradient Descent&quot;&gt;&lt;/a&gt;1. Gradient Descent&lt;/h2&gt;&lt;p&gt;Grad
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
</feed>
