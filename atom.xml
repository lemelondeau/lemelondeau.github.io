<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cool Site</title>
  
  
  <link href="//atom.xml" rel="self"/>
  
  <link href="https://lemelondeau.github.io/"/>
  <updated>2018-02-10T14:49:38.000Z</updated>
  <id>https://lemelondeau.github.io/</id>
  
  <author>
    <name>lemelondeau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Monte Carlo Sampling</title>
    <link href="https://lemelondeau.github.io/2018/02/10/mcmc/"/>
    <id>https://lemelondeau.github.io/2018/02/10/mcmc/</id>
    <published>2018-02-10T06:44:53.000Z</published>
    <updated>2018-02-10T14:49:38.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Sample-data-from-a-distribution"><a href="#1-Sample-data-from-a-distribution" class="headerlink" title="1. Sample data from a distribution"></a>1. Sample data from a distribution</h2><p>Sometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among all the distributions, the easiest one to sample from is uniform distribution $U(0,1)$. One thing worth noticing is most of the random generators generate pseudo random numbers rather than genuinely ones. Two commonly used methods are <strong><em>linear congruential generator</em></strong> (LCG) and <strong><em>multiplicative congruential generator</em></strong> (MCG):</p><script type="math/tex; mode=display">\text{LCG:}\left\{      \begin{array}{ll}      x_i=(C+\lambda x_{i-1} ) \text{mod}  M \\      r_i=x_i/M      \end{array}\right.</script><script type="math/tex; mode=display">\text{MCG:}\left\{      \begin{array}{ll}      x_i=\lambda x_{i-1} \text{mod} M\\      r_i=x_i/M      \end{array}\right.</script><p>where $x_0$ is called the seed, $C$ is a non-negative integer and $r_i$ is the generated random number. For further information, please refer to <a href="http://statweb.stanford.edu/~owen/mc/Ch-unifrng.pdf" target="_blank" rel="external">this</a>.</p><p>Now we have the random numbers from $U(0,1)$, how about other distributions? One well known method is <strong><em>inversion sampling</em></strong>.</p><p>Given a distribution $P(X)$, here $X$ is a continuous variable, we know that the output range of its cumulative distribution function $C(X)$ is $[0, 1]$. If we sample a random number $u$ from $U(0, 1)$, there will be always an $x$ such that $C(x)=u$. Now, $x$ is a sample from $P(X)$. To compute $x$ from $u$, we need the inverse function $C^{-1}$.</p><script type="math/tex; mode=display">x=C^{-1}(u)</script><p>What if $F(X)​$ is not invertible or we don’t even have the formula of $P(X)​$? </p><h2 id="2-Monte-Carlo-sampling"><a href="#2-Monte-Carlo-sampling" class="headerlink" title="2. Monte Carlo sampling"></a>2. Monte Carlo sampling</h2><p>In this part, we assume $p(x)$ is known.</p><h3 id="2-1-Rejection-sampling"><a href="#2-1-Rejection-sampling" class="headerlink" title="2.1 Rejection sampling"></a>2.1 Rejection sampling</h3><p>We want to sample data from a distribution $p(x)$, however, it is very difficult. Suppose we are lucky to have an easy-to-sample distribution $q(x)$, such that $kq(x) \geq p(x), k&gt;0$. $q(x)$ is called proposal distribution. What we are going to do is first sample a data $x_0$ from $q(x)$, and then sample $u_0$ from $U(0, kq(x_0))$. If $u_0&gt;p(x_0)$, the sample is rejected. This method is useful in one or two dimensions. [1]</p><h3 id="2-2-Importance-sampling"><a href="#2-2-Importance-sampling" class="headerlink" title="2.2 Importance sampling"></a>2.2 Importance sampling</h3><p>Suppose we have an easy-to-sample proposal distribution $q(x)$, such that $q(x)&gt;0$ if $p(x)&gt;0$. Now, we want to get the expectation of $f(x)$.</p><script type="math/tex; mode=display">\begin{align}E[f]&=\int f(x)p(x)\text{d}x\\&=\int f(x)\frac{p(x)}{q(x)}q(x)\text{d}x\\&\approx \frac1N\sum_n\frac{p(x_n)}{q(x_n)}q(x_n), x_n \sim q(x)\end{align}</script><p>$w_n=\frac{p(x_n)}{q(x_n)}$ is called <strong>importance weight</strong>. The above formula means sampling $f(x)$ from $p(x)$ is equivalent to sampling $f(x)w(x)$ from $q(x)$. All samples are retained, which is different from the reject sampling.</p><h3 id="2-3-Metropolis-Sampling"><a href="#2-3-Metropolis-Sampling" class="headerlink" title="2.3 Metropolis Sampling"></a>2.3 Metropolis Sampling</h3><p>The idea of Metropolis sampling is quite simple: from an initial position $x_0$, apply random walks to propose the next position and decide whether to accept it. If the new position $x_{t+1}$ is more likely to be visited than $x_t$ under $p(x)$, that is $p(x_{t+1})\geq p(x_t)$, the proposed position is accepted. Otherwise, accept the proposed position with probability $p(x_{t+1})/p(x_t)$, set $x_{t+1}$ to $x_t$. For the random walks, a Gaussian distribution is usually used: $p(x_{t+1}|x_t)\sim \mathcal{N}(x_t, 1)$.</p><p>I really hate some of the posts given the M-H algorithm (see section 2.4) first and say Metropolis sampling is a special case of it. Especially when they give the acceptance rate without further explanation, which makes me feel like I am the most stupid person in the world. </p><p>FYI: The Metropolis algorithm was first proposed in 1953. It was then generalized by Hastings in 1970. [2]</p><h3 id="2-4-Markov-Chain-Monte-Carlo-MCMC"><a href="#2-4-Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="2.4 Markov Chain Monte Carlo (MCMC)"></a>2.4 Markov Chain Monte Carlo (MCMC)</h3><p>What is not so good with Metropolis sampling? What is the intuition of MCMC?</p><h3 id="2-4-1-Markov-Chain"><a href="#2-4-1-Markov-Chain" class="headerlink" title="2.4.1 Markov Chain"></a>2.4.1 Markov Chain</h3><p>For Markov Chain, there are $n$ states $x_1, x_2, … ,x_n$ and the state at time $t+1$ is conditional independent to other historical states given the state at time $t$</p><script type="math/tex; mode=display">p(x_{t+1}|x_1, x_2,...,x_t)=p(x_{t+1}|x_t)</script><p>$p$ is called transition probability. And the transition function $T$:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t) \equiv p(x_{t+1}|x_t)</script><p>Starting form a certain state, we can get the distribution $\pi_t(x)$ at any time $t$ using the transition function. We call $\pi(x)$ a <strong>stationary distribution</strong> when it doesn’t change any more:</p><script type="math/tex; mode=display">\pi(x)=\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)</script><p>To make $\pi(x)$ stationary, a sufficient but not necessary condition is</p><script type="math/tex; mode=display">\pi(x^\prime)T(x\leftarrow x^\prime)=\pi(x)T(x^\prime \leftarrow x)</script><p>The above property is called <strong>detailed balance</strong> and we can prove the stationarity with this property:</p><script type="math/tex; mode=display">\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)=\sum_{x^\prime}\pi(x)T(x^\prime\leftarrow x)=\pi(x)</script><h3 id="2-4-2-Metropolis-Hastings-M-H-Algorithm"><a href="#2-4-2-Metropolis-Hastings-M-H-Algorithm" class="headerlink" title="2.4.2 Metropolis-Hastings (M-H) Algorithm"></a>2.4.2 Metropolis-Hastings (M-H) Algorithm</h3><p>M-H algorithm is one of the classical MCMC methods. In this section, we will see how to apply Markov Chain, especially it’s stationary property, to do sampling.</p><p><strong>A Markov chain view of Metropolis sampling</strong></p><p>Let’s say now we want to sampling from $\pi(x)$, we know that when it is stationary the next position $x_{t+1}$ we get by applying the transition function always has distribution $\pi(x)$. That means we can take $x_{t+1}$ as a sample of $\pi(x)$ ! Wow!</p><p>Now our problem is that we don’t know the exact $T$. What can we do?</p><p>Let’s look back at the Metropolis method in a Markov chain way. At position $x_t$, we want to know where will the next position be. In Markov chain, we get $x_{t+1}$ using $T(x_{t+1}\leftarrow x_t)$. In Metropolis method, there are two steps, first propose a new position using $Q(x_{t+1}\leftarrow x_t)$ and then decide whether to accept it using $A(x_{t+1}\leftarrow x_t)$. It’s now obvious that</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)</script><p>And if $x_{t+1}=x_t$, there are two parts, one if the proposal is itself and another is when the proposal is rejected:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)+\sum_{x^\prime}Q(x^\prime\leftarrow x_t)(1-A(x^\prime\leftarrow x_t))</script><p>We can specify the proposal distribution, what we don’t have is the acceptance rate $A(x_{t+1}\leftarrow x_t)$. How can we get it?</p><p><strong>Acceptance rate</strong></p><p>We hope that the chosen acceptance rate will give us a stationary distribution such that the states produced by the Markov chain are from the distribution $\pi(x)$. Applying the <strong>detailed balance</strong> we will get</p><script type="math/tex; mode=display">\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)=\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)</script><p>Subsequently, we can get</p><script type="math/tex; mode=display">%\frac{A(x^\prime \leftarrow x)}{A(x\leftarrow x^\prime)}=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}\\A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}A(x\leftarrow x^\prime)</script><p>Ideally, we want the acceptance rate to be <strong>as high as possible</strong>, so we choose $A(x\leftarrow x^\prime)$ to be 1 and $A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}$. But at the meanwhile, it is not larger than 1. So the final result would be</p><script type="math/tex; mode=display">A(x^\prime \leftarrow x)=\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)</script><p>We can prove the stationarity:</p><script type="math/tex; mode=display">\begin{align}\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)&=\pi(x)Q(x^\prime\leftarrow x)\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)\\&=\min(\pi(x^\prime)Q(x\leftarrow x^\prime), \pi(x)Q(x^\prime\leftarrow x)) \\&=\pi(x^\prime)Q(x\leftarrow x^\prime)\min(1, \frac{\pi(x)Q(x^\prime \leftarrow x)}{\pi(x^\prime)Q(x\leftarrow x^\prime)})\\&=\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)\end{align}</script><p>OK, now we can see why it is said Metropolis sampling is a special case of M-H algorithm. When $Q(x^\prime \leftarrow x)=Q(x \leftarrow x^\prime)$, M-H algorithm becomes Metropolis sampling.</p><p><strong>The algorithm</strong></p><ol><li>start from $x_0$</li><li>propose candidate $x^\prime$ position according to some proposal distribution $p(x^\prime|x)$</li><li>accept the candidate with probability $\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)$</li><li>if accepted, set $x_{t+1}=x^\prime$, otherwise $x_{t+1}=x_{t}$</li></ol><p><strong>Burn-in</strong></p><p>If the starting position is far from the dense area, it will take some time for the algorithm to reach the dense area (more representative of the distribution). Therefore, burn-in is needed.</p><p>Two good videos are <a href="https://www.youtube.com/watch?v=sK3cg15g8FI&amp;list=LLT7nvYvSbuz8tuy5ZqcCVYQ" target="_blank" rel="external">Machine learning - Markov chain Monte Carlo (MCMC) II</a> and<a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/hnzut/metropolis-hastings" target="_blank" rel="external">Metropolis-Hastings</a>.</p><h2 id="3-Approximate-Inference"><a href="#3-Approximate-Inference" class="headerlink" title="3. Approximate Inference"></a>3. Approximate Inference</h2><p>In practice, we usually want to sample from a distribution whose formula is unknown. The most common one is posterior $p(\theta|D)$.</p><script type="math/tex; mode=display">p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}</script><p>Usually, $p(D)=\int p(D|\theta)p(\theta)\text{d}\theta $ is intractable, how are we going to deal with this?</p><p>Notice that $p(D)$ is a constant and we are taking division so it doesn’t matter what it would be! This makes MCMC a powerful method for approximating posterior distributions.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>​    [1]. <a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf" target="_blank" rel="external">http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf</a></p><p>​    [2]. <a href="https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\" target="_blank" rel="external">https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\</a></p><p>​    [3]. <a href="http://www.maths.nuigalway.ie/~dane/Friel.pdf" target="_blank" rel="external">http://www.maths.nuigalway.ie/~dane/Friel.pdf</a></p><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Sample-data-from-a-distribution&quot;&gt;&lt;a href=&quot;#1-Sample-data-from-a-distribution&quot; class=&quot;headerlink&quot; title=&quot;1. Sample data from a dist
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Gradient Descent</title>
    <link href="https://lemelondeau.github.io/2018/01/28/SGD/"/>
    <id>https://lemelondeau.github.io/2018/01/28/SGD/</id>
    <published>2018-01-28T08:49:36.000Z</published>
    <updated>2018-02-03T06:53:45.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><p>Gradient descent is one of the methods for optimisation, especially when the objective function is convex. </p><ul><li><p>Gradient</p><p>The first question is, what is <strong>gradient</strong>? In mathematics, the gradient is a multivariate generalization of the derivative. </p><p>For a multivariate function $\varphi(x, y, z)$, the gradient can be written as the coordination form:</p><script type="math/tex; mode=display">\nabla \varphi(x, y, z)=\left(\frac{\partial\varphi}{\partial x},\frac{\partial\varphi}{\partial y},\frac{\partial\varphi}{\partial z}\right)</script><p>The direction of $\nabla\varphi $ is the orientation in which the <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> has the largest value and $|\nabla\varphi|$ is the value of that <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> [1]. If we consider a function of two variables, we can get a tangent plane at any point on the surface. There will be many directions along this plane,  and the direction with the largest slope is the gradient at this point. That is to say, the gradient at a particular point is the direction that the function value changes the fastest.</p></li><li><p>Gradient descent algorithm</p><p>Gradient descent algorithm can be used to find the local minimum of a differentiable  function $\varphi(\mathbf{\mu})$. After knowing what the gradient is, it’ll be easy to understand the gradient descent algorithm. Starting from an initial point $\mathbf{x}_0$ (usually this point is important), walk a small step $\eta$ (the step size is also important) along the opposite direction of the gradient, it will reach a new point, then restart from the new point. Repeat this procedure until the different in output after taking a step is smaller than the threshold $\eta$.<br>There are three parameters in this algorithm, the initial point, the step size, and the threshold respectively. Consider the sequence $\mathbf{\mu}_0, \mathbf{\mu}_1, \mathbf{\mu}_2, …$, such that</p><script type="math/tex; mode=display">\mathbf \mu_{t+1}=\mathbf \mu_t-\gamma\nabla\varphi(\mathbf \mu_t), t \geq 0</script></li><li><p>Gradient descent for linear regression</p><p>For linear regression, there are $n$ data points</p><script type="math/tex; mode=display">y_i=\mathbf x_i^T\mathbf w+b, i=1,2,...,n</script><p>The objective function to minimize is</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{n}\sum_{i=1}^{n}J_i(\theta)= \frac{1}{n}\sum_{i=1}^n J_i(\mathbf{w}, b)</script></li></ul><p>  Now, we can use the gradient descent algorithm to compute $\mathbf w$ and $b$, denote them as \theta$:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J(\mathbf \theta_t), t \geq 0</script><p>Code can be found <a href="https://github.com/lemelondeau/Algorithms/blob/master/SGD.ipynb" target="_blank" rel="external">here</a>.</p><h2 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h2><p>Many machine learning problems can be solved using gradient descent. However, when the dataset is very large, it will take a long time to update the  cost. Stochastic gradient descent (SGD) deals with this issue by updating the cost use only one sample each iteration:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J_i(\mathbf \theta_t), t \geq 0</script><p>Gradient descent:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \frac{1}{n}\sum_{i=1}^{n}\nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>SGD:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>The convergence of SGD is proven.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>​    [1]. <a href="http://mathworld.wolfram.com/Gradient.html" target="_blank" rel="external">http://mathworld.wolfram.com/Gradient.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Gradient-Descent&quot;&gt;&lt;a href=&quot;#1-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;1. Gradient Descent&quot;&gt;&lt;/a&gt;1. Gradient Descent&lt;/h2&gt;&lt;p&gt;Grad
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://lemelondeau.github.io/2017/10/14/hello-world/"/>
    <id>https://lemelondeau.github.io/2017/10/14/hello-world/</id>
    <published>2017-10-14T02:48:05.000Z</published>
    <updated>2017-10-14T07:12:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="test" scheme="https://lemelondeau.github.io/tags/test/"/>
    
  </entry>
  
</feed>
