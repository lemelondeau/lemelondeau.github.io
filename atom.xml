<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cool Site</title>
  
  
  <link href="//atom.xml" rel="self"/>
  
  <link href="https://lemelondeau.github.io/"/>
  <updated>2018-07-08T17:56:32.000Z</updated>
  <id>https://lemelondeau.github.io/</id>
  
  <author>
    <name>lemelondeau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Understanding Gaussian Process</title>
    <link href="https://lemelondeau.github.io/2018/07/09/GPs/"/>
    <id>https://lemelondeau.github.io/2018/07/09/GPs/</id>
    <published>2018-07-08T16:00:00.000Z</published>
    <updated>2018-07-08T17:56:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Informal-introduction-to-Gaussian-processes"><a href="#Informal-introduction-to-Gaussian-processes" class="headerlink" title="Informal introduction to Gaussian processes"></a>Informal introduction to Gaussian processes</h2><p>Gaussian processes (<strong>GPs</strong>) are used for supervised learning. It is entitled with the ability to present the uncertainty of prediction naturally. This model is originally from kriging. The basic idea of kriging is to predict the value of a function at a given point by computing a weighted average of the known values of the function in the neighbourhood of the point (wikipedia). This  idea  is very similar to k-nearest neighbourhood (KNN) algorithm. However, for computing the output of a new point, kriging uses all the observed data points and there will be weights related to them.  It is thus important to know the value of each weight, in other words, how two outputs are related.</p><p>I’d like to explain the GP model informally. </p><p>Given data $\mathcal D(\mathbf {X,y})$, we want to train a function $f$ such that one we have a new input $\mathbf x_i$, we can predict the output $\mathbf y_i=f(\mathbf x_i)$. It is nature to have the idea that when two inputs are close by, their outputs should also be similar. The next question is, which neighbour has more influence on a new point? The influence can also be considered as the similarity of two outputs. Since what we have for the new point is its input, we hope to define the similarity in the <strong>input space</strong>. Thus for any two outputs, the similarity can be measured. </p><p>This gives the rise of multivariate Gaussian, or joint Gaussian, where the similarity matrix is exactly the covariance matrix. We now regard the outputs as different variables in a multivariate Gaussian. It is said a Gaussian process is a generalization of the Gaussian probability distribution and is a distributions over <strong>functions</strong> [1]. This can be hard to understand at the first sight. Let’s consider an output $\mathbf y_i$, it is the value of $f$ at location $\mathbf x_i$, and it is also a random variable in this multivariate Gaussian. Once we have the covariance matrix, we get the similarity between any two outputs.  It’s hard to learn the covariance matrix directly. Therefore, we need a function to compute the covariance for any two variables. We call this function the <strong>kernel function</strong> or <strong>covariance function</strong>. From another point of view, the covariance matrix is positive definite, that means there is a kernel function for similarity measurement. Our aim now becomes to find the proper kernel to measure the similarity.</p><p>In the training process, we train the kernel and the objective is to maximise the likelihood. For prediction in regression, we use the rules for conditional Gaussian. Different from other regression methods, the results of prediction is not a single value, there will be a mean and a variance for each output. We are thus able to measure the uncertainty of the prediction.</p><p>The formal definition of Gaussian process is:</p><blockquote><p>A Gaussian process is a collection of random variables, any ﬁnite number of which have a joint Gaussian distribution. [1]</p></blockquote><p>For a thorough understanding of GP, read [1].</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Rasmussen, C. E., &amp; Williams, C. K. (2006). <em>Gaussian process for machine learning</em>. MIT press.</p><p>​    [2]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Informal-introduction-to-Gaussian-processes&quot;&gt;&lt;a href=&quot;#Informal-introduction-to-Gaussian-processes&quot; class=&quot;headerlink&quot; title=&quot;Inform
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Kernel methods</title>
    <link href="https://lemelondeau.github.io/2018/07/01/kernel/"/>
    <id>https://lemelondeau.github.io/2018/07/01/kernel/</id>
    <published>2018-07-01T08:37:28.000Z</published>
    <updated>2018-07-08T18:39:31.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Kernel methods</strong> are widely used in machine learning, such as in support vector machines (SVMs), Gaussian processes (GPs), and kernel principle component analysis (PCA). How does it get our favor? In plain English, a kernel function helps us project points in the current space to a <strong>feature space</strong> (usually high-dimensional)  and compute the inner product of feature vectors in this new space. What so good is, we don’t need a explicit function for the projection, the inner product is computed directly using this kernel!</p><p>There are four questions to answer. Why do we need the feature space? To which space the points are projected? What kind of functions can be used as a kernel function? How to find a proper kernel for a certain problem?</p><h2 id="Why-feature-space"><a href="#Why-feature-space" class="headerlink" title="Why feature space"></a>Why feature space</h2><p>In machine learning, we sometimes make a problem solvable by projecting the data points to another space. For example, in classification problem, the data points cannot be separated linearly. If we map those points to a high-dimensional space, they can be separated linearly. In linear regression, there are <em>basis functions</em> non-linear to the input. Those basis functions make the model more flexible and together they can be considered as a mapping function. </p><p>Let’s denote $\mathcal X$ as the original space and $\mathbf x$ as a data point in $\mathcal X$. Denote $\Phi$ as a mapping from $\mathcal X$ to a feature space $\mathcal F$.</p><h2 id="What-kind-of-feature-space"><a href="#What-kind-of-feature-space" class="headerlink" title="What kind of feature space"></a>What kind of feature space</h2><p>Computing $\Phi(\mathbf x)^T\Phi(\mathbf x^\prime)$ is often an important step after the projection. Now, suppose the feature space is very high-dimensional or even infinite-dimensional, it would be hard to get $\Phi(\mathbf x)^T\Phi(\mathbf x^\prime)$. How to tackle with this issue? Since all we want to get is the inner product (not the feature vectors) eventually, it’s <del>not very</del> natural to ask is there a function in the original space that gives the result of the inner product? This function can be defined as </p><script type="math/tex; mode=display">k(\mathbf x,\mathbf x^\prime)=\Phi(\mathbf x)^T\Phi(\mathbf x^\prime)=\langle \Phi(\mathbf x),\Phi(\mathbf x^\prime) \rangle_{\ell^2}</script><p>More generally, $\Phi$ might map the input to a Non-Euclidean space, where $\langle \Phi(\mathbf x),\Phi(\mathbf x^\prime) \rangle_{\mathcal F}$ is computed. This requires the feature space endowed with an inner-product operation. <strong>Hilbert spaces</strong> meet this requirement. Informally speaking, a Hilbert space is a vector space with norm and inner-product operation. Both $L^2$ and $\ell^2$ are Hilbert spaces.</p><!--You may ask, is there always a function $k$ and how to determine it? Not always. There will be a satisfactory function when the feature space is a reproducing kernel Hilbert space (**RKHS**). ?????? (It seems RKHS is a function space)--><p>The function $k$ is called <strong>kernel function</strong> and using a kernel function to represent the inner product in a feature space is called the <strong>kernel trick</strong>. Since inner product can measure the similarity, kernel function can also be considered as a method for similarity measure.</p><h2 id="Kernel-functions"><a href="#Kernel-functions" class="headerlink" title="Kernel functions"></a>Kernel functions</h2><p>What kinds of function can be used as the kernel function? The answer is when the <strong>Gram matrix</strong> $K$, defined by</p><script type="math/tex; mode=display">K_{ij}=k(\mathbf x_i,\mathbf x_j)</script><p>be positive definite for any set of inputs $( \mathbf x_i  )_{i=1}^N$.</p><p>If $K$ is positive definite, it can be written as $K=U^T\Lambda U=(\Lambda^{\frac 12}U) ^T(\Lambda^{\frac 12}U)=A^TA$. Then $K_{ij}=A_i^TA_j$. Define $\Phi(\mathbf x_i)=A_i$, then $K_{ij}=\Phi(\mathbf x_i)^T\Phi(\mathbf x_j)$. That means the function $k$ has a corresponding feature space $\mathcal F=R ^D$ which is the mapping from $\mathcal X$  using $\Phi$. We call $k$ a <strong>Mercer</strong> <strong>kernel</strong>, or <strong>positive deﬁnite kernel</strong>. A reproducing kernel Hilbert space (<strong>RKHS</strong>) is a space of functions related to a mercer kernel, it will be introduced latter. According to Moore-Aronszajn theorem, there is a one-to-one mapping between RKHS and positive definite kernels. </p><p>From another point of view, if $k$ can be written as the inner product of two feature vectors. Let $\mathcal H$ be a Hilbert space, $\Phi: \mathcal X \rightarrow \mathcal H$, then for any vector $\mathbf v$</p><script type="math/tex; mode=display">\begin{align}\mathbf v^TK\mathbf v&=\sum_i\sum_j  v_jk(\mathbf x_i,\mathbf x_j)v_i  \\&=\sum_i\sum_j v_j \Phi(\mathbf x_i)^T\Phi(\mathbf x_j)v_i \\& (?)=\sum_i\sum_j  \langle v_i \Phi(\mathbf x_i),v_j\Phi(\mathbf x_j)\rangle_{\mathcal H}  \\&=\left\langle \sum_i v_i\Phi(\mathbf x_i),\sum_j v_j\Phi(\mathbf x_j)\right\rangle _{\mathcal H}\\&=\left\|\sum_i v_i\Phi(\mathbf x_i)\right\|^2_{\mathcal H}\geq0\end{align}</script><p>That tells us, if $k$ is a valid kernel, $K$ must be positive definite.</p><p> (?) What’s the relationship between $\Phi(\mathbf x)^T\Phi(\mathbf x^\prime)$ and $\langle \Phi(\mathbf x),\Phi(\mathbf x^\prime) \rangle_{\mathcal H}$</p><h4 id="Mercer’s-condition-wikipedia"><a href="#Mercer’s-condition-wikipedia" class="headerlink" title="Mercer’s condition (wikipedia)"></a>Mercer’s condition (wikipedia)</h4><blockquote><p>A real-valued function is said to fulfil Mercer’s condition if for all <a href="https://en.wikipedia.org/wiki/Square-integrable_function" target="_blank" rel="external">square-integrable functions</a> <em>g</em>(<em>x</em>) one has</p><script type="math/tex; mode=display">\int\int g(x)k(x,y)g(y)dxdy \geq 0</script><p>The discrete form:</p><script type="math/tex; mode=display">\sum_{i,j} c_ic_j K_{ij} \geq 0</script></blockquote><h4 id="Constructing-kernels-PRML-6-2"><a href="#Constructing-kernels-PRML-6-2" class="headerlink" title="Constructing kernels (PRML 6.2)"></a>Constructing kernels (PRML 6.2)</h4><p>Choose a feature space mapping $\Phi$ and then use this to find the corresponding kernel.</p><p>Construct kernel functions directly.</p><p>Construct new kernels out of simpler kernels as building blocks.</p><p>Use a generative model.</p><p>Fisher kernel.</p><h2 id="Which-kernel-to-choose"><a href="#Which-kernel-to-choose" class="headerlink" title="Which kernel to choose"></a>Which kernel to choose</h2><p>Although for each RKHS space of $\mathcal X$ there exists a unique positive definite kernel, we usually don’t know which kernel is the most suitable one as the mapping function is implicit. There are numerous number of RKHS and therefore kernels. This makes the model very flexible. However, if a wrong kernel is chosen, we can’t get a good result. Kernel selection lies in the heart of machine learning models based on kernel function.</p><p>There are two understandings of kernel selection. One is to determine the parameters in the kernel function, a.k.a hyperparameters of the model. This can be trained by gradient-based methods. The other one is to choose the form of the kernel. It is traditionally done by comparing among kernel candidates and pick the best one, or selected by experts.</p><h2 id="RKHS-and-feature-space"><a href="#RKHS-and-feature-space" class="headerlink" title="RKHS and feature space"></a>RKHS and feature space</h2><p>According to <strong>Mercer’s theorem</strong>, if $k$ is a Mercer kernel (positive definite kernel), then there exists an infinite sequence of normalised eigenfunctions $(\phi_i)_{i=1}^\infty$ and eigenvalues $(\lambda_i)_{i=1}^\infty$, that we can write $k$ as</p><script type="math/tex; mode=display">k(\mathbf x, \mathbf x^\prime)=\sum_{i=1}^\infty \lambda_i\phi_i(\mathbf x)\phi_i(\mathbf x^\prime)</script><p>This is just  the infinite-dimensional analogue of the diagonalization of a symmetric matrix.</p><p>Use all the eigenfunctions as basis, we can get a function space, this is the reproducing kernel Hilbert space (<strong>RKHS</strong>). </p><script type="math/tex; mode=display">\mathcal H_k=\{ f: f(\mathbf x)=\sum_{i=1}^\infty f_i \phi_i(\mathbf x), \sum_{i=1}^\infty \frac{f_i^2}{\lambda_i} < \infty\}</script><p>Every function in RKHS is a linear combination of $(\phi_i)_{i=1}^\infty$, and $(f_i)_{i=1}^\infty$ is the coordinate in this space. The <strong>inner product</strong> in this space is <strong>defined</strong> as:</p><script type="math/tex; mode=display">\langle f, f^\prime \rangle_{\mathcal H_K}=\sum_{i=1}^\infty \frac{f_i f_i^\prime}{\lambda_i}</script><p>Now let’s consider the feature space, what’s the relationship between the above RKHS and the feature space $\mathcal H$ that $\Phi: \mathcal X \rightarrow \mathcal H$ ? Recall that $\Phi$ is a mapping corresponding to the kernel function $k$. According to the definition of kernel function, we have </p><script type="math/tex; mode=display">k(\mathbf x,\mathbf x^\prime)= \langle \Phi(\mathbf x),\Phi(\mathbf x^\prime)\rangle_{\mathcal H}</script><p>where $\Phi(\mathbf x)=k(\mathbf x, \cdot)$. The notion $f(\cdot)$ refers to the function itself.</p><p>To perform inner product in $\ell^2$ space, scale the coordinate by $\sqrt \lambda_i$, write $\Phi (\mathbf x)$ as</p><script type="math/tex; mode=display">\Phi(\mathbf x)=\left \langle \sqrt{\lambda_i}\phi_i(\mathbf x)\right\rangle_{i=0}^{\infty}</script><p>Now we have</p><script type="math/tex; mode=display">\begin{align}\left \langle \Phi(\mathbf x),\Phi(\mathbf x^\prime)\right\rangle_{\ell^2} &=\left \langle   \left \langle \sqrt{\lambda_i}\phi_i(\mathbf x)\right\rangle_i,   \left \langle \sqrt{\lambda_i}\phi_i(\mathbf x)\right\rangle_i \right\rangle _{\ell^2}\\&=\sum_{i=1}^\infty \lambda_i\phi_i(\mathbf x)\phi_i(\mathbf x^\prime)\\&=k(\mathbf x, \mathbf x^\prime)\end{align}</script><h4 id="The-reproducing-property"><a href="#The-reproducing-property" class="headerlink" title="The reproducing property"></a>The reproducing property</h4><p>Why this space is so called? </p><p>Define a special function in RKHS, $\mathbf y \in \mathcal X$</p><script type="math/tex; mode=display">K_\mathbf x (\mathbf y)=\sum_{i=1}^\infty \lambda_i\phi_i(\mathbf x)\phi_i(\mathbf y)=\sum_{i=1}^\infty g_i\phi_i(\mathbf y)</script><script type="math/tex; mode=display">\langle K_\mathbf x , K_\mathbf x^\prime  \rangle_{\mathcal H_k}=\sum_{i=1}^\infty\frac{g_i g_i^\prime}{\lambda_i}\\=\sum_{i=1}^\infty \frac{\lambda_i\phi_i(\mathbf x) \lambda_i\phi_i(\mathbf x^\prime)}{\lambda_i}\\=\sum_{i=1}^\infty \lambda_i\phi_i(\mathbf x)\phi_i(\mathbf x^\prime)\\=k(\mathbf x, \mathbf x^\prime)</script><p>This is called the reproducing property. $k$ is a inner product of two vectors.</p><script type="math/tex; mode=display">\langle K_\mathbf x , f  \rangle_{\mathcal H_k}=\sum_{i=1}^\infty\frac{g_i f_i}{\lambda_i}\\=\sum_{i=1}^\infty \frac{\lambda_i\phi_i(\mathbf x) f_i}{\lambda_i}\\=\sum_{i=1}^\infty f_i\phi_i(\mathbf x)=f(\mathbf x)</script><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. <a href="http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf" target="_blank" rel="external">http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf</a></p><p>​    [3]. <a href="http://legacydirs.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf" target="_blank" rel="external">http://legacydirs.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf</a></p><p>​    [4]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Kernel methods&lt;/strong&gt; are widely used in machine learning, such as in support vector machines (SVMs), Gaussian processes (GPs),
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>The sum-product algorithm</title>
    <link href="https://lemelondeau.github.io/2018/06/16/FG/"/>
    <id>https://lemelondeau.github.io/2018/06/16/FG/</id>
    <published>2018-06-16T15:36:06.000Z</published>
    <updated>2018-07-07T07:29:51.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Factor-graph-what-is-this-for"><a href="#1-Factor-graph-what-is-this-for" class="headerlink" title="1. Factor graph, what is this for?"></a>1. Factor graph, what is this for?</h2><p>On directed graphs, the joint distribution is written as the factorization of conditional distributions. Similarly, on undirected graphs, the joint distribution is the factorization of potential functions. Both the conditional distribution and the potential function are functions over subset of variables. Now, to make the factorization explicitly, additional nodes are introduced to represent the factors. Those new nodes are factor nodes, and the new bipartite graph is called factor graph.</p><p>There can be several difference factor graphs for the same graph. This allows factor graph to be more specific about the precise form of the factorization [1]. Also, there is no assumption that the factors in factor graph is corresponding to cliques or conditional independence properties of an underlying graph.</p><blockquote><p>Definition.[3]</p><p><em>A factor graph is a bipartite graph that expresses the structure of the factorization. The first part represents the variables, and the second part represents the functions. There is an edge from variable node $x_i$ to factor node $f_j$ if and only if $x_i$ is an argument of $f_j$.</em> </p></blockquote><h2 id="2-The-SUM-PRODUCT-algorithm"><a href="#2-The-SUM-PRODUCT-algorithm" class="headerlink" title="2. The SUM-PRODUCT algorithm"></a>2. The SUM-PRODUCT algorithm</h2><h3 id="2-1-Marginal-distribution-for-one-variable"><a href="#2-1-Marginal-distribution-for-one-variable" class="headerlink" title="2.1 Marginal distribution for one variable"></a>2.1 Marginal distribution for one variable</h3><p>The aim of the sum-product algorithm is to get the marginal distribution for every variable. Let $x_1, x_2,…,x_n$ be a collection of variables. By definition, the marginal is obtained by summing the joint distribution over all variables except $x$ so that</p><script type="math/tex; mode=display">p(x)=\sum_{\mathbf x \setminus x} p(\mathbf x)</script><p>If will solve this in a brute-force way, exponential number of steps will be taken.  Let’s see how a factor graph reduce the complexity of this problem. Hereafter, we assume the graph is a tree.</p><p>The joint distribution can be factorized as</p><script type="math/tex; mode=display">p(\mathbf x) =\prod _{s=1}^{S} f_s(x_{\mathcal{N}(s)})</script><p>where $S$ is the number of factors, $x_{\mathcal {N}(s)}$ denotes the neighbors of $f_s$ which are a subset of variables. </p><p>How does the factorization help to reduce the complexity? The distributive law is the key point.</p><p>Suppose the joint distribution has the form</p><script type="math/tex; mode=display">p(x_1, x_2, x_3, x_4)=f_a(x_1)f_b(x_1,x_2)f_c(x_2,x_3)f_d(x_2,x_4)f_e(x_3)f_f(x_4)</script><p>Now we want to get $p(x_1)$ which is</p><script type="math/tex; mode=display">p(x_1)=\sum_{x_2}\sum_{x_3}\sum _{x_4}f_a(x_1)f_b(x_1,x_2)f_c(x_2,x_3)f_d(x_2,x_4)f_e(x_3)f_f(x_4)</script><p>By using the distributive law, we can get</p><script type="math/tex; mode=display">p(x_1)=f_a(x_1) \left (\sum_{x_2}f_b(x_1,x_2)  \left (\sum_{x_3}f_c(x_2,x_3)f_e(x_3) \right) \left( \sum _{x_4}f_d(x_2,x_4)f_f(x_4)\right) \right)</script><p>Now, it’s less expensive to get the result. What’s more, from the above formula, we can see that there is message passing from $x_3,x_4$ to $x_2$ and then to $x_1$. The final message on $x_1$ involves all the information from other nodes. Let’s look into message passing.</p><h4 id="Message-passing"><a href="#Message-passing" class="headerlink" title="Message passing"></a><strong>Message passing</strong></h4><p>Messages that flow along the edges of graph are separated to two groups: messages flow from variable nodes to factor nodes $\mu_{x \rightarrow f}$, <!--this return is for displaying inline math,otherwise something is wrong--></p><p>and messages flow from factor nodes to variable nodes $\mu_{f \rightarrow x}$. All the message of previous nodes are marginalized, so the message from a certain node is a function of this node only. </p><p><img src="https://github.com/lemelondeau/notes/blob/master/resources/sum-product.png?raw=true" alt="sum-product.png"> </p><p>For the previous example, $p(x_1)$ can be written as</p><script type="math/tex; mode=display">p(x_1)=\mu_{f_a \rightarrow x_1}. \mu_{f_b \rightarrow x_1}</script><p>Further, $\mu_{f_b \rightarrow x_1}$ has the form</p><script type="math/tex; mode=display">\mu_{f_b \rightarrow x_1}=\sum_{x_2} f_b.\mu_{x_2\rightarrow f_b}=\sum_{n(f_b)\setminus x_1}\left( f_b\prod _{x \in n(f_b) \setminus x_1}\mu_{x\rightarrow f_b}\right)</script><p>where $n(f_b) $ denote the neighbor variable set of $f_b$.</p><p>Similarly, $\mu_{x_2\rightarrow f_b}$ has the form</p><script type="math/tex; mode=display">\mu_{x_2\rightarrow f_b} =\mu_{f_c \rightarrow x_2}.\mu_{f_d \rightarrow x_2}=\prod_{f \in n(x_2)\setminus f_b} \mu_{f \rightarrow x_2}</script><p>We can define the two types of message at this moment.</p><p><strong>Variable node to factor node:</strong></p><script type="math/tex; mode=display">\mu_{x\rightarrow f} =\prod_{f_l \in n(x)\setminus f} \mu_{f_l \rightarrow x}</script><p><strong>Factor node to variable node:</strong></p><script type="math/tex; mode=display">\mu_{f \rightarrow x}=\sum_{n(f_b)\setminus x}  \left( f \prod _{t \in n(f_b) \setminus x}\mu_{t\rightarrow f} \right)</script><p>For the leaf variable node, $\mu_{x\rightarrow f}=1$. </p><p>For the leaf factor node, $\mu_{f \rightarrow x}=f(x)$.</p><p>The marginal distribution for $x$ is</p><script type="math/tex; mode=display">p(x)=\prod_{f_s \in n(x)}\mu_{f_s\rightarrow x}</script><p>Take $x$ as the root, all the message will be passed from leaves to it. Message is passed according to the following protocol.</p><p><strong>Message-Passing Protocol.</strong> A node can send a message to a neighboring mode when and only when it has received messages from all of its neighbors.</p><h3 id="2-2-Marginal-distribution-for-all-variables"><a href="#2-2-Marginal-distribution-for-all-variables" class="headerlink" title="2.2 Marginal distribution for all variables"></a>2.2 Marginal distribution for all variables</h3><p>If we want to get the marginals for every variable, we can run the above algorithm for each of them. However, doing so will cause many repeated computation. We can pick any of the variable node as root, propagate messages from the leaves to the root as before. The the root node can send message to all its neighbors. Thereafter, the message will be passes down all the way to the leaves. Since every variable node will have received message from all of its neighbors, we can readily calculate the marginal distribution for each of them. We only need to compute messages twice as the number of edges. </p><p><strong>Code in C++ can be found <a href="https://github.com/lemelondeau/Algorithms/blob/master/sum-product.cpp" target="_blank" rel="external">here</a> (Can’t believe I used C++ three years ago), the <a href="https://github.com/lemelondeau/Algorithms/blob/master/sum-product%20problem.png" target="_blank" rel="external">problem</a> is from NUS CS5340 2015/2016 Sem1.</strong></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. Jordan, Michael I (2003). “An introduction to probabilistic graphical models.” </p><p>​    [3]. <a href="https://lipn.univ-paris13.fr/~dovgal/2016/03/03/sum-product-algorithm-part1.html" target="_blank" rel="external">https://lipn.univ-paris13.fr/~dovgal/2016/03/03/sum-product-algorithm-part1.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Factor-graph-what-is-this-for&quot;&gt;&lt;a href=&quot;#1-Factor-graph-what-is-this-for&quot; class=&quot;headerlink&quot; title=&quot;1. Factor graph, what is this 
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Bayesian Linear Regression and Gaussian Process</title>
    <link href="https://lemelondeau.github.io/2018/05/10/BLR/"/>
    <id>https://lemelondeau.github.io/2018/05/10/BLR/</id>
    <published>2018-05-10T11:48:29.000Z</published>
    <updated>2018-05-23T06:08:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML</em></strong></p><h2 id="1-Linear-Basis-Function-Models"><a href="#1-Linear-Basis-Function-Models" class="headerlink" title="1. Linear Basis Function Models"></a>1. Linear Basis Function Models</h2><h3 id="1-1-Model-Formalisation"><a href="#1-1-Model-Formalisation" class="headerlink" title="1.1 Model Formalisation"></a>1.1 Model Formalisation</h3><p>Linear regression is one of the most basic regression models for  supervised learning. It is a <strong>parametric model</strong>, that is to say, the number of parameters to be determined is fixed. During training, the objective of linear regression is to minimise the difference between real output and predicted output. Or equivalently, to maximise the likelihood function. </p><p>The simplest linear model for regression is</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+...+w_Dx_D=\mathbf{w}^T\mathbf{x}</script><p>where $\mathbf{x}=(x_1, … ,x_D)T$ and $\mathbf{w}=(w_0, w_1, … .w_D)$ which is the parameter. It is clear that the output $y$ is a linear combination of the input variables. The key property of this model is that <strong>it is a linear function of the parameters</strong> [1]. Even if the model have the form</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+w_1x_1^2+...+w_Dx_D^2 ,</script><p>it is still a linear regression model. This property encourages us to extend the model to be more flexible by considering linear combinations of fixed nonlinear functions of the input variables, with the following form</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=w_0+\sum _{j=1}^{M-1} w_j\phi_j(\mathbf{x})</script><p>where $\phi_j(\mathbf{x})$ are known as basis functions, $M$ is the number of parameters. Define $\phi_0(\mathbf x)=1$, we get</p><script type="math/tex; mode=display">y(\mathbf{x},\mathbf{w})=\sum_{j=0}^{M-1}w_j\phi_j{\mathbf x}=\mathbf w^T\pmb{\phi}(\mathbf x)</script><p>By using nonlinear basis functions, we allow the function $y(\mathbf{x},\mathbf{w})$ to be a non-linear function of the input vector $\mathbf x$.</p><h3 id="1-2-Model-Fitting"><a href="#1-2-Model-Fitting" class="headerlink" title="1.2 Model Fitting"></a>1.2 Model Fitting</h3><p>Given a dataset $\mathcal{D}(\mathbf{X,t})$, usually the observations are noisy. Assume the target variable $t$ is the noisy observation</p><script type="math/tex; mode=display">t=y(\mathbf{x},\mathbf{w})+\epsilon</script><p>where $\epsilon \sim \mathcal N(0, \beta^{-1})$. Thus we can write</p><script type="math/tex; mode=display">p(t|\mathbf x, \mathbf w, \beta)=\mathcal N(t|y(\mathbf{x},\mathbf{w}), \beta^{-1})</script><p><strong>Least Square Method</strong></p><p>The squared error is written as</p><script type="math/tex; mode=display">E_D(\mathbf w)= \frac{1}{2}\sum_{n=1}^{N}\{t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n)\}^2</script><p>Take the derivative respect to $\mathbf w$ and make it to be zero, we can get</p><script type="math/tex; mode=display">\mathbf w=(\Phi^T\Phi)^{-1}\Phi^T\mathbf t</script><p><strong>Maximum Likelihood</strong> </p><p>For i.i.d variables, the likelihood function of target variables is</p><script type="math/tex; mode=display">p(\mathbf t|\mathbf X, \mathbf w, \beta)=\prod_{n=1}^{N}\mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})</script><p>Taking the logarithm of the likelihood function </p><script type="math/tex; mode=display">\begin{align}\ln p(\mathbf t|\mathbf w, \beta)&=\sum_{n=1}^{N}\ln \mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})\\&=\sum_{n=1}^{N}\ln (\frac{\sqrt\beta}{\sqrt{2\pi}}\exp(-(t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n))^2\beta/2))\\&=\sum_{n=1}^{N}(\frac12\ln\beta-\frac12\ln(2\pi)-\frac12\beta\{t_n-\mathbf w^T\pmb{\phi}(\mathbf x_n)\}^2)\\&=\frac N2\ln \beta-\frac N2\ln(2\pi)-\beta E_D(\mathbf w)\end{align}</script><p>Maximising the likelihood is therefore equivalent to minimising the square error.</p><h2 id="2-Bayesian-Linear-Regression"><a href="#2-Bayesian-Linear-Regression" class="headerlink" title="2. Bayesian Linear Regression"></a>2. Bayesian Linear Regression</h2><p>Over-fitting is a big issue when using maximum likelihood, Bayesian treatment is considered to avoid this.</p><h3 id="2-1-Posterior-Distribution-of-Parameter"><a href="#2-1-Posterior-Distribution-of-Parameter" class="headerlink" title="2.1 Posterior Distribution of Parameter"></a>2.1 Posterior Distribution of Parameter</h3><p>One of the aim of Bayesian linear regression is to get the distribution of parameter $\mathbf w$, which is written as $p(\mathbf w|\mathbf X, \mathbf t)$. This is a posterior distribution. Assume a Gaussian distribution over $\mathbf w$,</p><script type="math/tex; mode=display">p(\mathbf w)=\mathcal N(\mathbf w|\mathbf{m_0},\mathbf{S_0}).</script><p>The likelihood is also easy to get</p><script type="math/tex; mode=display">\begin{align}p(\mathbf t|\mathbf X, \mathbf w, \beta)&=\prod_{n=1}^{N}\mathcal N(t_n|\mathbf w^T\pmb{\phi}(\mathbf x_n), \beta^{-1})\\&=\mathcal N(\mathbf t|\Phi \mathbf w, \beta^{-1}I)\end{align}</script><p>Apply 2.116 conditional Gaussian we will get</p><script type="math/tex; mode=display">p(\mathbf w|\mathbf X, \mathbf t)=\mathcal N(\mathbf w|\mathbf{m}_N,\mathbf{S}_N).</script><p>where</p><script type="math/tex; mode=display">\mathbf m_N=\mathbf S_N(\mathbf S_0^{-1}\mathbf m_0+\beta\Phi^T\mathbf t)\\\mathbf S_N^{-1}=\mathbf S_0^{-1}+\beta\Phi^T\Phi</script><p>Now, assign a parameter $\alpha$ to control the prior distribution</p><script type="math/tex; mode=display">p(\mathbf w|\alpha)=\mathcal N(\mathbf w|\mathbf 0, \alpha^{-1}I)</script><p>and the corresponding posterior distribution is given by</p><script type="math/tex; mode=display">\mathbf m_N=\beta\mathbf S_N\Phi^T\mathbf t\\\mathbf S_N^{-1}=\alpha I+\beta\Phi^T\Phi</script><h3 id="2-2-Predictive-Distribution"><a href="#2-2-Predictive-Distribution" class="headerlink" title="2.2 Predictive Distribution"></a>2.2 Predictive Distribution</h3><p>To make prediction for new values of $\mathbf x$, we need to integrate $\mathbf w$ out. The predictive distribution is given by</p><script type="math/tex; mode=display">p(t|\mathbf x, \mathcal D,\alpha, \beta)=\int p(t|\mathbf x, \mathbf w, \beta)p(\mathbf w|\mathcal D,\alpha, \beta)\text{d}\mathbf w\\=\int \mathcal N(t|\mathbf w^T\phi (\mathbf x) ,\beta^{-1})\mathcal N(\mathbf w|\mathbf{m}_N,\mathbf{S}_N)\text{d}\mathbf w\\=\mathcal N(t|\mathbf m_N^T\phi(\mathbf x), \sigma_N^2(\mathbf x))</script><p>where $\sigma_N^2$ is given by (use 2.115 marginal distribution)</p><script type="math/tex; mode=display">\sigma_N^2(\mathbf x)=\frac1\beta+\phi(\mathbf x)^T\mathbf S_N\phi(\mathbf x)</script><h2 id="3-Relation-with-Gaussian-Process"><a href="#3-Relation-with-Gaussian-Process" class="headerlink" title="3. Relation with Gaussian Process"></a>3. Relation with Gaussian Process</h2><p>In the above Bayesian linear regression model, $\alpha$ and $\beta$ govern the predictive distribution. We call $\alpha$ and $\beta$ <strong>hyperparameters</strong>.</p><p>The hidden variables is given by</p><script type="math/tex; mode=display">y =y(\mathbf{x},\mathbf{w})=\sum_{j=0}^{M-1}w_j\phi_j{\mathbf x}=\mathbf w^T\pmb{\phi}(\mathbf x)</script><p>$\mathbf y$ is a linear combination of Gaussian distributed variables hence is itself Gaussian. The mean and covariance is given by</p><script type="math/tex; mode=display">\mathbb{E}[y] = \phi(\mathbf x)^T\mathbb{E}(\mathbf w)=\phi(\mathbf x)^T\mathbf{m}_N,\\\text{cov}[y,y']=\mathbb E [(y-\mathbb{E}[y])(y'-\mathbb{E}[y'])] =\phi(\mathbf x)^T\mathbf{S}_N\phi(\mathbf x^\prime),</script><p>We already have </p><script type="math/tex; mode=display">\mathbf m_N=\beta\mathbf S_N\Phi^T\mathbf t\\\mathbf S_N^{-1}=\alpha I+\beta\Phi^T\Phi</script><p>therefore,</p><script type="math/tex; mode=display">\mathbb E[y]=\beta\phi(\mathbf x)^T\mathbf S_N\Phi^T\mathbf t \\\text{var}[y]=\phi(\mathbf x)^T\mathbf S_N\phi(\mathbf x)</script><p>The mean and variance can be rewritten as following (though I didn’t try)</p><script type="math/tex; mode=display">\mathbb E[y]=\phi(\mathbf x)^T\Sigma_p\Phi(K+\beta^{-1}I)^{-1}\mathbf t\\\text{var}[y]=\phi(\mathbf x)^T\Sigma_p\phi(\mathbf x)-\phi(\mathbf x)^T\Sigma_p\Phi(K+\beta^{-1}I)^{-1}\Phi^T\Sigma_p\phi(\mathbf x)</script><p>where $\Sigma_p=\alpha^{-1} I$ and $K=\Phi^T \Sigma_p\Phi$.</p><p>Define $k(\mathbf x, \mathbf x^{\prime})=\phi(\mathbf x)^T\Sigma_p\phi(\mathbf x^{\prime})$, we can easily see Bayesian linear regression is equivalent to a Gaussian Process model, whose mean and variance of the predictive distribution are</p><script type="math/tex; mode=display">\pmb{\mu}=k(\mathbf x, \mathbf X)^T(K+\beta^{-1}I)^{-1}\mathbf t\\\Sigma=k(\mathbf x, \mathbf x)-k(\mathbf x, \mathbf X)^T (K+\beta^{-1}I)^{-1}k(\mathbf x, \mathbf X)</script><h2 id=""><a href="#" class="headerlink" title=""></a><!--4. The Evidence Approximation--></h2><!--marginalisation over hyperparameters--><!--laplace approximation--><!--EM--><!--log marginal likelihood--><p>(Finish writing on 2018-05-23 T^T)</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. Rasmussen, C. E., &amp; Williams, C. K. (2006). <em>Gaussian process for machine learning</em>. MIT press.</p><p>​    [3]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-Linear-Basis-Function-Models&quot;&gt;&lt;a href=&quot;#1-Linear-Basis-Function-Models&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Propagation</title>
    <link href="https://lemelondeau.github.io/2018/04/01/EP/"/>
    <id>https://lemelondeau.github.io/2018/04/01/EP/</id>
    <published>2018-04-01T06:07:08.000Z</published>
    <updated>2018-06-16T15:58:03.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML</em></strong></p><p>Expectation propagation (EP) is another deterministic approximate inference method besides variational inference (VI). The different between them is one minimises $KL(p||q)$ while another minimises $KL(q||p)$.</p><h2 id="1-KL-divergence"><a href="#1-KL-divergence" class="headerlink" title="1.KL divergence"></a>1.KL divergence</h2><script type="math/tex; mode=display">KL(p||q)=\int p(x)\log \frac{p(x)}{q(x)}\text{d}(x)</script><p>Where $p$ is the real distribution and $q$ is the approximation we want to find.  If we want to minimise $KL(p||q)$, we need to make sure when $p(x)$ is large, $q(x)$ also need to be large ($q(x)$ is non-zero when $p(x)$ is non-zero). On the other hand, if we want to minimise $KL(q||p)$, we need to make sure when $p(x)$ is close to zero, $q(x)$ is also near zero.</p><p><img src="https://github.com/lemelondeau/notes/blob/master/resources/KL.png?raw=true" alt="KL.png"></p><p>Figure 1. The blue contours show the real distribution $p$ and we want to approximate it with a Gaussian distribution. (a) The red contours represent a distribution minimises $KL(p||q)$. (b)(c) The red contours show distributions minimise $KL(q||p)$. [2]</p><h2 id="2-Sufficient-statistics-and-Exponential-family"><a href="#2-Sufficient-statistics-and-Exponential-family" class="headerlink" title="2. Sufficient statistics and Exponential family"></a>2. Sufficient statistics and Exponential family</h2><h3 id="2-1-Sufficient-statistics-1"><a href="#2-1-Sufficient-statistics-1" class="headerlink" title="2.1 Sufficient statistics [1]"></a>2.1 Sufficient statistics [1]</h3><p>Suppose we have a random sample $\textbf{x}_1,…,\textbf{x}_n$ taken from a distribution $f(\textbf{x}|\theta)$ which relies on an unknown parameter $\theta$ in a parameter space $\Theta$. The purpose of parameter estimation is to estimate the parameter $\theta$ from the random sample. Any function of the random sample $\textbf{x}_1,…,\textbf{x}_n$ is called a <strong>statistic</strong>. Mean and variance functions are both statistics.</p><p>If a statistic $T$ contains all the information to estimate $\theta$, then we say $T$ is <strong>sufficient</strong>. That is to say, given $T=t$, the random sample $\textbf{x}_1,…,\textbf{x}_n$ and parameter $\theta$ are independent of each other.</p><h5 id="Factorization-theorem"><a href="#Factorization-theorem" class="headerlink" title="Factorization theorem:"></a>Factorization theorem:</h5><p>A statistic $T(\textbf{x}_1,…,\textbf{x}_n)$ is a sufficient statistic for θ if and only if the joint pdf or the joint point mass function $f(\textbf{X}|\theta)$ of $\textbf{x}_1,…,\textbf{x}_n$ can be factorized as follows: </p><script type="math/tex; mode=display">f(\textbf{X}|\theta)=\mu(\textbf{X})\upsilon[T(\textbf{X}),\theta]</script><p>Here, the function $\mu$ and $\upsilon$ are nonnegative, the function $\mu$ may depend on $\textbf{x}$ but does not depend on $\theta$, and the function $\upsilon$ depends on $\theta$ but will depend on the observed value $\textbf{x}$ only through the value of the statistic $T(\textbf x)$.</p><h3 id="2-2-Exponential-family"><a href="#2-2-Exponential-family" class="headerlink" title="2.2 Exponential family"></a>2.2 Exponential family</h3><script type="math/tex; mode=display">p(\textbf x|\pmb\eta)=h(\textbf x)g(\pmb\eta)\exp\{\pmb \eta^T \textbf u(\textbf x)\}</script><script type="math/tex; mode=display">p(\textbf X|\pmb \eta)=\left(\prod_{n=1}^Nh(\textbf x_n)\right)g(\pmb \eta)^N\exp \left\{\pmb \eta^T \sum_{n=1}^N\textbf u(\textbf x)\right\}</script><p>According to the factorisation theorem, $\sum_{n=1}^N\textbf u(\textbf x)$ is the <em>sufficient statistic</em> of the distribution in exponential family. Therefore, $\mathbb E[\textbf u(\textbf x)]$ is also a sufficient statistic.</p><p>Taking the gradient of both sides of the following equation:</p><script type="math/tex; mode=display">g(\pmb \eta)\int h(\textbf x)\exp\{\pmb \eta^T \textbf u(\textbf x)\}\text d\textbf x=1</script><p>we will obtain</p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E[\textbf u(\textbf x)]</script><h2 id="3-Expectation-Propagation"><a href="#3-Expectation-Propagation" class="headerlink" title="3. Expectation Propagation"></a>3. Expectation Propagation</h2><p>Suppose $\mathcal D$ is observed data and $\textbf z$ is hidden variable (including parameters), we would like to know the posterior over $\textbf z$, $p(\textbf z|D)$. The aim of expectation propagation (EP) is to approximate a distribution $p(\textbf z)$ with another distribution $q(\textbf z)$ which is a member of the exponential family. The problem is to minimise $KL(p||q)$:</p><script type="math/tex; mode=display">KL(p||q)=-\ln g(\pmb \eta)-\pmb \eta^T\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]+\text{const}</script><p>We can minimise $ KL(p||q)$ by setting the gradient w.r.t. $\pmb \eta$ to zero and this results in</p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]</script><p>We have seen in section 2 that </p><script type="math/tex; mode=display">-\nabla \ln g(\pmb \eta)=\mathbb E_{q(\textbf z)}[\textbf u(\textbf z)]</script><p>Therefore,</p><script type="math/tex; mode=display">\mathbb E_{q(\textbf z)}[\textbf u(\textbf z)]=\mathbb E_{p(\textbf z)}[\textbf u(\textbf z)]</script><p>That is to say, the optimum solution corresponds to matching the expected sufficient statistics. This is called <strong>moment matching</strong>.</p><p>Unfortunately, at this moment, averaging with respect to  the true distribution is intractable.</p><h5 id="A-product-of-factors"><a href="#A-product-of-factors" class="headerlink" title="A product of factors:"></a>A product of factors:</h5><p>Let’s look back at the posterior distribution itself.</p><script type="math/tex; mode=display">p(\textbf z|D)=\frac{p(D, \textbf z)}{p(D)}</script><p>where the joint distribution can be write as a product of factors in the form (given the data is i.i.d):</p><script type="math/tex; mode=display">p(D, \textbf z)=p(\textbf z)\prod_i f_i(\textbf z)</script><p>For each datapoint $\textbf x_i$, there is a factor $f_i(\textbf z)=p(\textbf x_i|\textbf z)$. $p(\textbf z)$ corresponds to the prior.</p><p>The approximation to the posterior distribution is also given by a product of factors</p><script type="math/tex; mode=display">q(\textbf z)=\frac 1Z\prod_i \widetilde{f}_i(\textbf z)</script><p>The KL divergence to minimise is now given by</p><script type="math/tex; mode=display">KL(p\|q)=KL\left(\frac{1}{p(D)}p(\textbf z)\prod_i f_i(\textbf z) \|\frac 1Z\prod_i \widetilde{f}_i(\textbf z)\right)</script><p>Instead of minimising the KL divergence w.r.t the true distribution, we can perform the minimisation between the corresponding pairs $f_i(\textbf z)$ and $\tilde f_i(\textbf z)$.</p><blockquote><p>Expectation Propagation</p><ol><li><p>Initialize all the approximating factors $\tilde f_i(\textbf z)$</p></li><li><p>Initialize the posterior approximation by setting</p><script type="math/tex; mode=display">q(\textbf z) \propto \prod_i \tilde f_i(\textbf z)</script></li><li><p>Until convergence:</p><p>a. Choose a factor $\tilde f_j(\textbf z)$ to refine.</p><p>b. Remove $\tilde f_j(\textbf z)$ from the posterior by division and get $q^{\setminus j}(\textbf z)$</p><p>c. Evaluate the new posterior by setting the sufficient statistics of $q^{\text{new}}(\textbf z)$ equal to those of $q^{\setminus j}(\textbf z)f_j(\textbf z)$, $(f_j(\textbf z)=p(\textbf x_j|\textbf z))$, evaluate the normalisation constant</p><script type="math/tex; mode=display">Z_j=\int q^{\setminus j}(\textbf z)f_j(\textbf z) \text d\textbf z</script><p>d. Evaluate and store the new factor</p><script type="math/tex; mode=display">\tilde f_j(\textbf z)=Z_j\frac{q^{\text{new}}(\textbf z)}{q^{\setminus j}(\textbf z)}</script></li></ol><!--belief propagation--> </blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. <a href="http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Sufficient.pdf" target="_blank" rel="external">http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Sufficient.pdf</a></p><p>​    [2]. Christopher M. Bishop. <em>Pattern Recognition and Machine Learning</em>. Springer, 2006.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Expectation propagation (EP) is another deterministic approximate inference method besides
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>The EM algorithm</title>
    <link href="https://lemelondeau.github.io/2018/03/17/EM/"/>
    <id>https://lemelondeau.github.io/2018/03/17/EM/</id>
    <published>2018-03-16T16:00:00.000Z</published>
    <updated>2019-09-17T13:29:28.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>Notes of PRML and Machine Learning-A Probabilistic Prospective</em></strong></p><h2 id="1-Latent-Variables"><a href="#1-Latent-Variables" class="headerlink" title="1. Latent Variables"></a>1. Latent Variables</h2><p><em>Latent variables</em>, also called <em>hidden variables</em> or <em>unobserved variables</em>, are used in many probabolistic models to make it more tractable. Sometimes the observed variables are noisy, so there is a latent variable for each observed variable, such as in Gaussian process.</p><script type="math/tex; mode=display">y=f+\epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>Where $y$ is the observed variable and $f$ is a latent variable.</p><p>Sometimes there is no noise but it is difficult to observe certain variables directly, whereas there are some observable variables related to them. The change of hidden variables will be reflected by the observable variables. For example, the variables in the hidden Markov model.</p><p>Latent variables can also be used to reduce the dimensionality of data, where the data points all lie close to a manifold of much lower dimensionality than that of the original data space [1].</p><p>Denote $X$ as observed variables and $Z$ as latent variables. A common used formula to ease the intractable problem using latent variable is</p><script type="math/tex; mode=display">P(X)=\int_ZP(X,Z)</script><h2 id="2-The-Expectation-Maximization-Algorithm"><a href="#2-The-Expectation-Maximization-Algorithm" class="headerlink" title="2. The Expectation Maximization Algorithm"></a>2. The Expectation Maximization Algorithm</h2><h3 id="2-1-Gaussian-Mixture-Model"><a href="#2-1-Gaussian-Mixture-Model" class="headerlink" title="2.1 Gaussian Mixture Model"></a>2.1 Gaussian Mixture Model</h3><p>Gaussian distribution has some important analytical properties, but it suffers from signiﬁcant limitations when it comes to modelling real data sets [1]. For example, it is impossible to model a multimodal distribution. However, when we superpose several Gaussian distributions, the above mentioned limitation is overcome. The Gaussian mixture model (<strong>GMM</strong>) with $K$ components has the form</p><script type="math/tex; mode=display">p(\textbf{x})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>Where $\sum \pi_k=1$ and $\mathbf{\mu}_k, \mathbf{\Sigma}_k$ are mean and variance for the $k^{\text{th}}$ component, and these are the parameters to be computed.</p><p><strong>The intractable derivative of likelihood function</strong></p><p>If we use the MLE method to estimate the parameters, the log likelihood function is given by</p><script type="math/tex; mode=display">\log p(\mathbf{X}|\mathbf{\pi,\mu,\Sigma})=\sum_{n=1}^N\log \left\{\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}_n|\mathbf{\mu}_k, \mathbf{\Sigma}_k)\right\}</script><p>where $\mathbf{X}={\mathbf{x}_1,\mathbf{x}_2,…,\mathbf{x}_N}$.</p><p>Usually, if we want to maximize the log likelihood, we take the partial derivative over the parameters. Unfortunately, there is a summation of logarithm, making it impossible to find a closed-form analytical solution. One way to solve this problem is to apply gradient-based optimisation techniques [1]. An alternative approach is the expectation maximization (<strong>EM</strong>) algorithm.</p><h3 id="2-2-Gaussian-Mixture-Model-with-Latent-Variables"><a href="#2-2-Gaussian-Mixture-Model-with-Latent-Variables" class="headerlink" title="2.2 Gaussian Mixture Model with Latent Variables"></a>2.2 Gaussian Mixture Model with Latent Variables</h3><p>A $K$-dimensional binary random variable $\mathbf{z}$ is introduced. It has a $1$-of-$K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0, we can have</p><script type="math/tex; mode=display">p(\mathbf{z})=\prod_{k=1}^K \pi_k^{z_k}</script><p>The joint distribution</p><script type="math/tex; mode=display">p(\mathbf{x,z})=p(\mathbf{x|z})p(\mathbf z)</script><p>The marginal distribution of $\mathbf{x}$ is given by</p><script type="math/tex; mode=display">p(\mathbf{x})=\sum_\mathbf{z}p(\mathbf z)p(\mathbf{x|z})=\sum_{k=1}^K \pi_k \mathcal{N}(\textbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)</script><p>We are now able to work with the joint distribution instead of the marginal distribution, and this will lead to significant simplification [1].</p><h3 id="2-3-The-EM-Algorithm"><a href="#2-3-The-EM-Algorithm" class="headerlink" title="2.3 The EM Algorithm"></a>2.3 The EM Algorithm</h3><p>Denote all parameters by $\pmb{\theta}$, and the log likelihood function is given by</p><script type="math/tex; mode=display">\log p(\mathbf{X|\pmb\theta})=\log \left\{\sum_{\mathbf{Z} } p(\mathbf{Z|\pmb\theta})p(\mathbf{X,Z|\pmb\theta})\right\}</script><p>Now consider the joint distribution for GMM,</p><script type="math/tex; mode=display">p(\mathbf{X,Z|\pmb\theta})=\prod_{n=1}^N\prod_{k=1}^K\pi_k^{z_{nk}}\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)^{z_{nk}}</script><p>where $z_{nk}$ denotes the $k^{\text{th}}$ components of $\mathbf{z}_n$. Taking the logarithm, we obtain</p><script type="math/tex; mode=display">\log p(\mathbf{X,Z|\pmb\theta})=\sum_{n=1}^N\sum_{k=1}^K z_{nk} \{\log\pi_k+\log\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Sigma}_k)\}</script><p>The logarithm now acts directly on the Gaussian distribution. The summation is out of the log. The  maximization with respect to a mean or a covariance is exactly for a single Gaussian.</p><p>The EM algorithm is given as follows:</p><p><strong>E step</strong>: find the expectation of the $\log p(\mathbf{X, Z}|\pmb\theta)$ over the posterior distribution of the latent variable</p><script type="math/tex; mode=display">\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})=\sum_{\mathbf{Z}}p(\mathbf{Z|X, \pmb\theta}^{\text{old}})\log p(\mathbf{X,Z}|\pmb\theta)</script><p><strong>M step</strong>: maximize $\mathcal{Q}$</p><script type="math/tex; mode=display">\pmb{\theta}^{\text{new}}={\arg\max}_{\pmb\theta}\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})</script><p><strong>What the hell does this mean? Where does the $\mathcal{Q}$ come from? Why does this equal to maximize $p(\mathbf{X|\pmb\theta})$? I felt I was eating a fly…</strong></p><p>We need to read further.</p><h2 id="3-EM-and-VI"><a href="#3-EM-and-VI" class="headerlink" title="3.EM and VI"></a>3.EM and VI</h2><h3 id="3-1-The-EM-Algorithm-in-General"><a href="#3-1-The-EM-Algorithm-in-General" class="headerlink" title="3.1 The EM Algorithm in General"></a>3.1 The EM Algorithm in General</h3><blockquote><p>The expectation maximization algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables.</p><p>Suppose that direct optimisation of $p(\mathbf{X}|\pmb{\theta})$ is difficult, but that optimisation of the complete-data likelihood function $p(\mathbf{X,Z|\pmb\theta})$ is significantly easier.</p></blockquote><p>To compute $p(\mathbf{X|\pmb\theta})=\sum_{\mathbf Z} p(\mathbf{Z})p(\mathbf{X,Z|\pmb\theta})$, the posterior distribution of $\mathbf Z$, $p(\mathbf{Z|X,\pmb\theta})$, is needed.</p><blockquote><p>Next introduce a distribution $q(\mathbf{Z})$ defined over the latent variable, which is supposed to be as close as possible to $p(\mathbf{Z|X,\pmb\theta})$.</p></blockquote><p>The following decomposition holds</p><script type="math/tex; mode=display">\log p(\mathbf{X|\pmb\theta})=\mathcal{L}(q, \pmb{\theta})+\text{KL}(q||p)</script><p>where we have defined</p><script type="math/tex; mode=display">\mathcal{L}(q,\pmb \theta)=\sum_{\mathbf Z}q(\mathbf Z)\log\left\{\frac{p(\mathbf{X,Z}|\pmb\theta)}{q(\mathbf{Z})}\right\} \\\text{KL}(q||p)= - \sum_{\mathbf{Z} }\log \left\{\frac{p(\mathbf{Z|X,\pmb\theta})}{q(\mathbf Z)}\right\}</script><p>Since $\text{KL}(q||p) \geq 0$, then $\log p(\mathbf{X|\pmb\theta}) \geq \mathcal{L}(q, \pmb{\theta})$. $\mathcal L$ is a lower bound of the log likelihood.</p><p>Our aim is to maximize $\log p(\mathbf{X|\pmb\theta})$ hence $\mathcal L$ and there are two group of variables, $\mathbf Z$ and $\pmb\theta$, to optimise.</p><p>Suppose the current value of the parameter vector is $\pmb \theta^\text {old}$</p><p><strong>E step:</strong> Fix $\pmb \theta^\text {old}$, maximize $\mathcal L$ w.r.t $q(\mathbf Z)$. <strong>Since $\pmb \theta^\text {old}$ is fixed,</strong> <strong>then $\log p(\mathbf{X|\pmb\theta})$ is fixed.</strong> The largest value of $\mathcal L$ will occur when $\text{KL}$ equals to zero. That means $q(\mathbf{Z})$ is equal to $p(\mathbf{Z|X},\pmb\theta^\text{old})$.</p><p><strong>M step:</strong> Fix $q(\mathbf Z) = p(\mathbf{Z|X},\pmb\theta^\text{old})$, maximize $\mathcal L$ w.r.t $\pmb \theta$. </p><script type="math/tex; mode=display">\begin{align}\mathcal L(q,\pmb\theta)&=\sum_\mathbf{Z}p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{X,Z}|\pmb\theta)-\sum_\mathbf Z p(\mathbf{Z|X},\pmb\theta^\text{old})\log p(\mathbf{Z|X},\pmb\theta^\text{old})\\&=\mathcal{Q}(\pmb{\theta}, \pmb\theta^{\text{old}})+\text{const}\end{align}</script><p><strong>Here comes the $\mathcal Q$ that is mentioned before.</strong></p><p>For an i.i.d data set,</p><script type="math/tex; mode=display">p(\mathbf{Z|X},\pmb\theta)=\frac{p(\mathbf{Z},\mathbf{X}|\pmb\theta)}{\sum_\mathbf{Z} p(\mathbf{Z},\mathbf{X}|\pmb\theta)}=\frac{\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}{\sum_\mathbf{Z}\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)}=\prod_n p(\mathbf{x}_n,\mathbf{z}_n|\pmb\theta)</script><h3 id="3-2-EM-and-VI"><a href="#3-2-EM-and-VI" class="headerlink" title="3.2 EM and VI"></a>3.2 EM and VI</h3><p>See <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">this article</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. Christopher M. Bishop (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><p>​    [2]. Robert, C. (2014). <em>Machine learning, a probabilistic perspective.</em></p><p>​    [3]. <a href="https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/" target="_blank" rel="external">https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Notes of PRML and Machine Learning-A Probabilistic Prospective&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-Latent-Variables&quot;&gt;&lt;a href=&quot;#1-Late
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="https://lemelondeau.github.io/2018/02/24/VI/"/>
    <id>https://lemelondeau.github.io/2018/02/24/VI/</id>
    <published>2018-02-24T03:48:47.000Z</published>
    <updated>2018-10-23T04:34:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Understanding-Variational-Inference"><a href="#1-Understanding-Variational-Inference" class="headerlink" title="1. Understanding Variational Inference"></a>1. Understanding Variational Inference</h2><p>I had a difficult time understanding what does the phrase <em>Variational Inference</em> mean. First, let’s see what is <em>inference</em>. It refers to <em>statistical inference</em> which means to estimate or <strong>infer</strong> the parameters/hidden variables of a statistical model. For example, estimating the weights in linear regression. In general, there are two different statistical inference methods, namely <em>Frequentist method</em> and <em>Bayesian inference</em>. Still use the linear regression as an example, frequentist method will give a estimated value for each weight while the Bayesian inference will produce a distribution over the weights. What is going to be used in variational inference is Bayesian inference, where posterior distribution is derived by the Bayes’ rule.</p><p>What does <em>variational</em> refer to? Variation is a concept in <em>functional</em> ( mappings from a set of functions to the real numbers. It’s to find a function which minimise or maximise the functional. If we want to get the optima of a function, usually we can take the derivative, which is called differential. Similarly, if we take the derivative of a functional, it is called variational.</p><p>As we know, posterior distribution is notoriously intractable. In variational inference, we aim to find an approximation to it. During this process, a functional is maximised, where the input of the optima is the approximation mentioned just now.</p><h2 id="2-Tackle-Posterior-with-Variational-Inference"><a href="#2-Tackle-Posterior-with-Variational-Inference" class="headerlink" title="2. Tackle Posterior with Variational Inference"></a>2. Tackle Posterior with Variational Inference</h2><h3 id="2-1-Problem-Formalisation"><a href="#2-1-Problem-Formalisation" class="headerlink" title="2.1 Problem Formalisation"></a>2.1 Problem Formalisation</h3><p>Suppose we have observations $\mathbf X ={\mathbf x_1, \mathbf x_2,…,\mathbf x_n}$ and latent random variables $\mathbf Z={\mathbf z_1,\mathbf z_2,…,\mathbf z_n}$. Here, the latent random variables include the parameters of model and the latent variables whose counterparts are the observations. We are going to compute $P(\mathbf Z|\mathbf X)$, the posterior, which is</p><script type="math/tex; mode=display">p(\mathbf{Z}|\mathbf{X})=\frac{p(\mathbf{X}|\mathbf{Z})p(\mathbf{Z})}{\int p(\mathbf{X},\mathbf{Z})p(\mathbf{Z}) d\mathbf Z}</script><p>However, for many models, the denominator may not have a closed-form solution and the latent variable might be high dimensional. Therefore, we want to find an approximation $q(\mathbf{Z})​$ of $p(\mathbf{Z}|\mathbf{X})​$, such that $KL(q||p)​$ is minimised. We assume $q(\mathbf Z)​$ a Gaussian distribution.</p><script type="math/tex; mode=display">\begin{align}KL(q||p) &=KL(q(\mathbf{Z})||p(\mathbf Z|\mathbf X)) \\ &=\int q(\mathbf Z)\log \frac{q(\mathbf Z)}{p(\mathbf Z|\mathbf X)} d\mathbf Z\\&=\int q(\mathbf Z) \log \frac{q(\mathbf Z)p(\mathbf X)}{p(\mathbf {X},\mathbf{Z})}d \mathbf Z\\&=\int q(\mathbf Z)[\log q(\mathbf Z)+\log p(\mathbf X)-\log p(\mathbf  X, \mathbf  Z)] d \mathbf Z\\&=\int q(\mathbf  Z)[\log q(\mathbf Z)-\log p(\mathbf X,\mathbf Z)]d \mathbf Z+\log p(\mathbf X)\end{align}  \qquad(1)</script><h3 id="2-2-The-evidence-lower-bound-ELBO"><a href="#2-2-The-evidence-lower-bound-ELBO" class="headerlink" title="2.2 The evidence lower bound(ELBO)"></a>2.2 The evidence lower bound(ELBO)</h3><p>Rearrange the above equation (1), we get</p><script type="math/tex; mode=display">\log p(\mathbf X)=KL(q||p)-\int q(\mathbf  Z)[\log q(\mathbf Z)-\log p(\mathbf X,\mathbf Z)]d \mathbf Z</script><p>That is </p><script type="math/tex; mode=display">\log p(\mathbf X)=KL(q||p)+E_{q(\mathbf Z)}[\log p(\mathbf X, \mathbf Z)-\log q(\mathbf Z)]</script><p>Since $KL(q||p)\geq 0$ and $\log p(\mathbf X)$ is a constant, then minimizing $KL(q||p)$ is equivalent to maximizing the second term. Also, the second term is called the <em>evidence lower bound (ELBO)</em>:</p><script type="math/tex; mode=display">\mathcal L(p)=\int q(\mathbf Z)\log \frac {p(\mathbf X, \mathbf Z)}{q(\mathbf Z)} d \mathbf Z \qquad\qquad\qquad(2)</script><p>This is the <strong>functional</strong> we want to maximise.</p><blockquote><script type="math/tex; mode=display">\mathcal L(p)=E_{q(\mathbf Z)}[\log p(\mathbf X, \mathbf Z)-\log q(\mathbf Z)]\\=E_{q(\mathbf Z)}[\log p(\mathbf X|\mathbf Z)+\log p(\mathbf Z)-\log q(\mathbf Z)]\\=E_{q(\mathbf Z)}[\log p(\mathbf X|\mathbf Z)]-KL(q(\mathbf Z)||p(\mathbf Z))</script><p>The first term is an expected likelihood; it encourages densities that place their mass on configurations of the latent variables that explain the observed data. The second term is the negative divergence between the variational density and the prior; it encourages densities close to the prior. Thus the variational objective mirrors the usual balance between likelihood and prior. [5]</p></blockquote><p>The lower bound can also be obtained by applying Jensen’s inequality:</p><script type="math/tex; mode=display">\begin{align}\log p(\mathbf{X})&=\log\int p(\mathbf X, \mathbf Z) d \mathbf{Z}\\&=\log \int p(\mathbf X,\mathbf Z)\frac{q(\mathbf Z)}{q(\mathbf Z)} d \mathbf Z\\&=\log (E_q\left[ \frac{p(\mathbf X,\mathbf Z)}{q(\mathbf Z)}\right])\\&\geq E_q\left[\log p(\mathbf X,\mathbf Z)-q(\mathbf Z)\right]\\&=\int q(\mathbf Z)\log \frac {p(\mathbf X, \mathbf Z)}{q(\mathbf Z)} d \mathbf Z\end{align}</script><h3 id="2-3-Maximising-the-ELBO-and-Get-the-Approximation"><a href="#2-3-Maximising-the-ELBO-and-Get-the-Approximation" class="headerlink" title="2.3 Maximising the ELBO and Get the Approximation"></a>2.3 Maximising the ELBO and Get the Approximation</h3><p>We consider a restricted family of distributions $q(\mathbf Z)$ and then seek the member of this family for which the KL divergence is minimized. Our goal is to restrict the family sufﬁciently that they comprise only tractable distributions, while at the same time allowing the family to be sufﬁciently rich and ﬂexible that it can provide a good approximation to the true posterior distribution [1].</p><h4 id="2-3-1-Mean-field-variational-inference"><a href="#2-3-1-Mean-field-variational-inference" class="headerlink" title="2.3.1 Mean field variational inference"></a>2.3.1 Mean field variational inference</h4><p>Partition the elements of $\mathbf Z$ into disjoint groups $\mathbf Z_i$ where $i=1,2,…M$. Then the factorization of $q$ is</p><script type="math/tex; mode=display">q(\mathbf Z)=\prod_{i=1}^M q_i(\mathbf Z_i)</script><p>Then the ELBO can be refactorized as</p><script type="math/tex; mode=display">\begin{align}\mathcal L&=\int q(\mathbf Z) \log p(\mathbf X, \mathbf Z)d \mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\int q(\mathbf Z) \log[p(\mathbf X)\prod_i p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\log p(\mathbf X)+\int q(\mathbf Z) \log[\prod_ip(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\&=\log p(\mathbf X)+\int q(\mathbf Z) \sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z-\int q(\mathbf Z) \sum_i\log q_i d\mathbf Z\\\end{align} \qquad(3)</script><p>Consider the partitions $\mathbf Z={\mathbf Z_j, \overline{\mathbf Zj}}$ where $ \overline{\mathbf Zj}=\mathbf Z\setminus \mathbf Z_j $.</p><script type="math/tex; mode=display">\begin{align}\int q(\mathbf Z)\sum_i \log q_i d\mathbf Z&=\int \prod_i q_i\sum_i\log q_i d\mathbf Z\\&=\sum_i \int \left( \prod_i q_i\right)\log q_i d\mathbf Z\\&=\sum_i \int \int q_j \overline{q_j} \log q_i d\mathbf Z_j d\mathbf {\overline{\mathbf Z}}_ j\end{align}</script><p>Let $i=j$, then the above formula</p><script type="math/tex; mode=display">\begin{align}&=\sum_i \int \int q_i \overline{q_i} \log q_i d\mathbf Z_i d\mathbf {\overline{\mathbf Z}}_i\\&=\sum_i \int \log q_i d\mathbf Z_i \\\end{align}</script><p>Similarly,</p><script type="math/tex; mode=display">\begin{align}\int q(\mathbf Z) \sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z&=\int \int q_i \overline{q_i}\sum_i\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z_id\overline{\mathbf Z_i}\\&=\sum_i \int \int q_i \overline{q_i}\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\mathbf Z_id\overline{\mathbf Z_i}\\&=\sum_i \int \left(\int \overline{q_i}\log[p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]d\overline{\mathbf Z_i}\right) q_i d\mathbf Z_i\\&=\sum_i E_i\left[E_{-i}[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]\right]\\&=\sum_i E[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]\end{align}</script><p>So we can get</p><script type="math/tex; mode=display">\mathcal L=\log p(\mathbf X)+\sum_i E[\log p(\mathbf Z_i|\mathbf Z_{1:i-1},\mathbf X)]-\sum_i \int \log q_i d\mathbf Z_i \\</script><h4 id="2-3-2-Coordinate-Ascent"><a href="#2-3-2-Coordinate-Ascent" class="headerlink" title="2.3.2 Coordinate Ascent"></a>2.3.2 Coordinate Ascent</h4><p>Now we fix all the elements in $\mathbf Z$ except for $\mathbf Z_j$ whose is the last variable in the above chain rule list.</p><p>Write the objective as a function of $q_j=q(\mathbf Z_j)$</p><script type="math/tex; mode=display">\mathcal L_j=\int q_j E_{-j}[\log p(\mathbf Z_i|\mathbf Z_{-j},\mathbf X)]d\mathbf Z_j-\int q_j\log q_j d\mathbf Z_j+ \text {const}</script><p>Take the derivative of $\mathcal L_j$ (<strong>variational</strong>),</p><script type="math/tex; mode=display">\frac{\mathcal L_j}{d q_j}=E_{-j}[\log p(\mathbf Z_i|\mathbf Z_{-j},\mathbf X)]- \log q_j-1=0</script><script type="math/tex; mode=display">q_j^* \propto \exp\left\{E_{-j}[\log p(\mathbf Z_j|\mathbf Z_{-j}, \mathbf X)]\right\}</script><p>the denominator of the posterior does not depend the $\mathbf Z_j$, so</p><script type="math/tex; mode=display">q_j^* \propto \exp\left\{E_{-j}[\log p(\mathbf Z_j,\mathbf Z_{-j}, \mathbf X)]\right\}</script><p>Update $q_j$ iteratively until the convergence of ELBO.</p><h2 id="3-Variational-Inference-vs-MCMC"><a href="#3-Variational-Inference-vs-MCMC" class="headerlink" title="3. Variational Inference vs MCMC"></a>3. Variational Inference vs MCMC</h2><blockquote><p>MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density (Robert and Casella, 2004). Variational inference does not enjoy such guarantees—it can only find a density close to the target—but tends to be faster than MCMC. Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. [5]</p></blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. springer.</p><p>[2] <a href="https://benmoran.wordpress.com/2015/02/21/variational-bayes-and-the-evidence-lower-bound/" target="_blank" rel="external">https://benmoran.wordpress.com/2015/02/21/variational-bayes-and-the-evidence-lower-bound/</a></p><p>[3] <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf" target="_blank" rel="external">https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf</a></p><p>[4] Fox, C. W., &amp; Roberts, S. J. (2012). A tutorial on variational Bayesian inference. <em>Artificial intelligence review</em>, 1-11.</p><p>[5] Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017). Variational inference: A review for statisticians. <em>Journal of the American Statistical Association</em>, <em>112</em>(518), 859-877.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Understanding-Variational-Inference&quot;&gt;&lt;a href=&quot;#1-Understanding-Variational-Inference&quot; class=&quot;headerlink&quot; title=&quot;1. Understanding V
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Monte Carlo Sampling</title>
    <link href="https://lemelondeau.github.io/2018/02/10/mcmc/"/>
    <id>https://lemelondeau.github.io/2018/02/10/mcmc/</id>
    <published>2018-02-10T06:44:53.000Z</published>
    <updated>2018-02-10T15:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Sample-data-from-a-distribution"><a href="#1-Sample-data-from-a-distribution" class="headerlink" title="1. Sample data from a distribution"></a>1. Sample data from a distribution</h2><p>Sometimes we need a set of data from a certain distribution, but the difficulty lies in this problem depends on the distribution we are going to sample from. Among all the distributions, the easiest one to sample from is uniform distribution $U(0,1)$. One thing worth noticing is most of the random generators generate pseudo random numbers rather than genuinely ones. Two commonly used methods are <strong><em>linear congruential generator</em></strong> (LCG) and <strong><em>multiplicative congruential generator</em></strong> (MCG):</p><script type="math/tex; mode=display">\text{LCG:}\left\{      \begin{array}{ll}      x_i=(C+\lambda x_{i-1} ) \text{mod}  M \\      r_i=x_i/M      \end{array}\right.</script><script type="math/tex; mode=display">\text{MCG:}\left\{      \begin{array}{ll}      x_i=\lambda x_{i-1} \text{mod} M\\      r_i=x_i/M      \end{array}\right.</script><p>where $x_0$ is called the seed, $C$ is a non-negative integer and $r_i$ is the generated random number. For further information, please refer to <a href="http://statweb.stanford.edu/~owen/mc/Ch-unifrng.pdf" target="_blank" rel="external">this</a>.</p><p>Now we have the random numbers from $U(0,1)$, how about other distributions? One well known method is <strong><em>inversion sampling</em></strong>.</p><p>Given a distribution $P(X)$, here $X$ is a continuous variable, we know that the output range of its cumulative distribution function $C(X)$ is $[0, 1]$. If we sample a random number $u$ from $U(0, 1)$, there will be always an $x$ such that $C(x)=u$. Now, $x$ is a sample from $P(X)$. To compute $x$ from $u$, we need the inverse function $C^{-1}$.</p><script type="math/tex; mode=display">x=C^{-1}(u)</script><p>What if $F(X)​$ is not invertible or we don’t even have the formula of $P(X)​$? </p><h2 id="2-Monte-Carlo-sampling"><a href="#2-Monte-Carlo-sampling" class="headerlink" title="2. Monte Carlo sampling"></a>2. Monte Carlo sampling</h2><p>In this part, we assume $p(x)$ is known.</p><h3 id="2-1-Rejection-sampling"><a href="#2-1-Rejection-sampling" class="headerlink" title="2.1 Rejection sampling"></a>2.1 Rejection sampling</h3><p>We want to sample data from a distribution $p(x)$, however, it is very difficult. Suppose we are lucky to have an easy-to-sample distribution $q(x)$, such that $kq(x) \geq p(x), k&gt;0$. $q(x)$ is called proposal distribution. What we are going to do is first sample a data $x_0$ from $q(x)$, and then sample $u_0$ from $U(0, kq(x_0))$. If $u_0&gt;p(x_0)$, the sample is rejected. This method is useful in one or two dimensions. [1]</p><h3 id="2-2-Importance-sampling"><a href="#2-2-Importance-sampling" class="headerlink" title="2.2 Importance sampling"></a>2.2 Importance sampling</h3><p>Suppose we have an easy-to-sample proposal distribution $q(x)$, such that $q(x)&gt;0$ if $p(x)&gt;0$. Now, we want to get the expectation of $f(x)$.</p><script type="math/tex; mode=display">\begin{align}E[f]&=\int f(x)p(x)\text{d}x\\&=\int f(x)\frac{p(x)}{q(x)}q(x)\text{d}x\\&\approx \frac1N\sum_n\frac{p(x_n)}{q(x_n)}q(x_n), x_n \sim q(x)\end{align}</script><p>$w_n=\frac{p(x_n)}{q(x_n)}$ is called <strong>importance weight</strong>. The above formula means sampling $f(x)$ from $p(x)$ is equivalent to sampling $f(x)w(x)$ from $q(x)$. All samples are retained, which is different from the reject sampling.</p><h3 id="2-3-Metropolis-Sampling"><a href="#2-3-Metropolis-Sampling" class="headerlink" title="2.3 Metropolis Sampling"></a>2.3 Metropolis Sampling</h3><p>The idea of Metropolis sampling is quite simple: from an initial position $x_0$, apply random walks to propose the next position and decide whether to accept it. If the new position $x_{t+1}$ is more likely to be visited than $x_t$ under $p(x)$, that is $p(x_{t+1})\geq p(x_t)$, the proposed position is accepted. Otherwise, accept the proposed position with probability $p(x_{t+1})/p(x_t)$, set $x_{t+1}$ to $x_t$. For the random walks, a Gaussian distribution is usually used: $p(x_{t+1}|x_t)\sim \mathcal{N}(x_t, 1)$.</p><p>I really hate some of the posts given the M-H algorithm (see section 2.4) first and say Metropolis sampling is a special case of it. Especially when they give the acceptance rate without further explanation, which makes me feel like I am the most stupid person in the world. </p><p>FYI: The Metropolis algorithm was first proposed in 1953. It was then generalized by Hastings in 1970. [2]</p><h3 id="2-4-Markov-Chain-Monte-Carlo-MCMC"><a href="#2-4-Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="2.4 Markov Chain Monte Carlo (MCMC)"></a>2.4 Markov Chain Monte Carlo (MCMC)</h3><p>What is not so good with Metropolis sampling? What is the intuition of MCMC?</p><h3 id="2-4-1-Markov-Chain"><a href="#2-4-1-Markov-Chain" class="headerlink" title="2.4.1 Markov Chain"></a>2.4.1 Markov Chain</h3><p>For Markov Chain, there are $n$ states $x_1, x_2, … ,x_n$ and the state at time $t+1$ is conditional independent to other historical states given the state at time $t$</p><script type="math/tex; mode=display">p(x_{t+1}|x_1, x_2,...,x_t)=p(x_{t+1}|x_t)</script><p>$p$ is called transition probability. And the transition function $T$:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t) \equiv p(x_{t+1}|x_t)</script><p>Starting form a certain state, we can get the distribution $\pi_t(x)$ at any time $t$ using the transition function. We call $\pi(x)$ a <strong>stationary distribution</strong> when it doesn’t change any more:</p><script type="math/tex; mode=display">\pi(x)=\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)</script><p>To make $\pi(x)$ stationary, a sufficient but not necessary condition is</p><script type="math/tex; mode=display">\pi(x^\prime)T(x\leftarrow x^\prime)=\pi(x)T(x^\prime \leftarrow x)</script><p>The above property is called <strong>detailed balance</strong> and we can prove the stationarity with this property:</p><script type="math/tex; mode=display">\sum_{x^\prime}\pi(x^\prime)T(x\leftarrow x^\prime)=\sum_{x^\prime}\pi(x)T(x^\prime\leftarrow x)=\pi(x)</script><h3 id="2-4-2-Metropolis-Hastings-M-H-Algorithm"><a href="#2-4-2-Metropolis-Hastings-M-H-Algorithm" class="headerlink" title="2.4.2 Metropolis-Hastings (M-H) Algorithm"></a>2.4.2 Metropolis-Hastings (M-H) Algorithm</h3><p>M-H algorithm is one of the classical MCMC methods. In this section, we will see how to apply Markov Chain, especially it’s stationary property, to do sampling.</p><p><strong>A Markov chain view of Metropolis sampling</strong></p><p>Let’s say now we want to sampling from $\pi(x)$, we know that when it is stationary the next position $x_{t+1}$ we get by applying the transition function always has distribution $\pi(x)$. That means we can take $x_{t+1}$ as a sample of $\pi(x)$ ! Wow!</p><p>Now our problem is that we don’t know the exact $T$. What can we do?</p><p>Let’s look back at the Metropolis method in a Markov chain way. At position $x_t$, we want to know where will the next position be. In Markov chain, we get $x_{t+1}$ using $T(x_{t+1}\leftarrow x_t)$. In Metropolis method, there are two steps, first propose a new position using $Q(x_{t+1}\leftarrow x_t)$ and then decide whether to accept it using $A(x_{t+1}\leftarrow x_t)$. It’s now obvious that</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)</script><p>And if $x_{t+1}=x_t$, there are two parts, one if the proposal is itself and another is when the proposal is rejected:</p><script type="math/tex; mode=display">T(x_{t+1}\leftarrow x_t)=Q(x_{t+1}\leftarrow x_t)A(x_{t+1}\leftarrow x_t)+\sum_{x^\prime}Q(x^\prime\leftarrow x_t)(1-A(x^\prime\leftarrow x_t))</script><p>We can specify the proposal distribution, what we don’t have is the acceptance rate $A(x_{t+1}\leftarrow x_t)$. How can we get it?</p><p><strong>Acceptance rate</strong></p><p>We hope that the chosen acceptance rate will give us a stationary distribution such that the states produced by the Markov chain are from the distribution $\pi(x)$. Applying the <strong>detailed balance</strong> we will get</p><script type="math/tex; mode=display">\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)=\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)</script><p>Subsequently, we can get</p><script type="math/tex; mode=display">%\frac{A(x^\prime \leftarrow x)}{A(x\leftarrow x^\prime)}=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}\\A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}A(x\leftarrow x^\prime)</script><p>Ideally, we want the acceptance rate to be <strong>as high as possible</strong>, so we choose $A(x\leftarrow x^\prime)$ to be 1 and $A(x^\prime \leftarrow x)=\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)}$. But at the meanwhile, it is not larger than 1. So the final result would be</p><script type="math/tex; mode=display">A(x^\prime \leftarrow x)=\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)</script><p>We can prove the stationarity:</p><script type="math/tex; mode=display">\begin{align}\pi(x)Q(x^\prime \leftarrow x)A(x^\prime \leftarrow x)&=\pi(x)Q(x^\prime\leftarrow x)\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)\\&=\min(\pi(x^\prime)Q(x\leftarrow x^\prime), \pi(x)Q(x^\prime\leftarrow x)) \\&=\pi(x^\prime)Q(x\leftarrow x^\prime)\min(1, \frac{\pi(x)Q(x^\prime \leftarrow x)}{\pi(x^\prime)Q(x\leftarrow x^\prime)})\\&=\pi(x^\prime)Q(x\leftarrow x^\prime)A(x\leftarrow x^\prime)\end{align}</script><p>OK, now we can see why it is said Metropolis sampling is a special case of M-H algorithm. When $Q(x^\prime \leftarrow x)=Q(x \leftarrow x^\prime)$, M-H algorithm becomes Metropolis sampling.</p><p><strong>The algorithm</strong></p><ol><li>start from $x_0$</li><li>propose candidate $x^\prime$ position according to some proposal distribution $p(x^\prime|x)$</li><li>accept the candidate with probability $\min(\frac{\pi(x^\prime)Q(x\leftarrow x^\prime)}{\pi(x)Q(x^\prime \leftarrow x)},1)$</li><li>if accepted, set $x_{t+1}=x^\prime$, otherwise $x_{t+1}=x_{t}$</li></ol><p><strong>Burn-in</strong></p><p>If the starting position is far from the dense area, it will take some time for the algorithm to reach the dense area (more representative of the distribution). Therefore, burn-in is needed.</p><p>Two good videos are <a href="https://www.youtube.com/watch?v=sK3cg15g8FI&amp;list=LLT7nvYvSbuz8tuy5ZqcCVYQ" target="_blank" rel="external">Machine learning - Markov chain Monte Carlo (MCMC) II</a> and<a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/hnzut/metropolis-hastings" target="_blank" rel="external">Metropolis-Hastings</a>.</p><h2 id="3-Approximate-Inference"><a href="#3-Approximate-Inference" class="headerlink" title="3. Approximate Inference"></a>3. Approximate Inference</h2><p>In practice, we usually want to sample from a distribution whose formula is unknown. The most common one is posterior $p(\theta|D)$.</p><script type="math/tex; mode=display">p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}</script><p>Usually, $p(D)=\int p(D|\theta)p(\theta)\text{d}\theta $ is intractable, how are we going to deal with this?</p><p>Notice that $p(D)$ is a constant and we are taking division so it doesn’t matter what it would be! This makes MCMC a powerful method for approximating posterior distributions.</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>​    [1]. <a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf" target="_blank" rel="external">http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf</a></p><p>​    [2]. <a href="https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\" target="_blank" rel="external">https://www.sheffield.ac.uk/polopoly_fs/1.60510!/file/MCMC.pdf\</a></p><p>​    [3]. <a href="http://www.maths.nuigalway.ie/~dane/Friel.pdf" target="_blank" rel="external">http://www.maths.nuigalway.ie/~dane/Friel.pdf</a></p><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Sample-data-from-a-distribution&quot;&gt;&lt;a href=&quot;#1-Sample-data-from-a-distribution&quot; class=&quot;headerlink&quot; title=&quot;1. Sample data from a dist
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Gradient Descent</title>
    <link href="https://lemelondeau.github.io/2018/01/28/SGD/"/>
    <id>https://lemelondeau.github.io/2018/01/28/SGD/</id>
    <published>2018-01-28T08:49:36.000Z</published>
    <updated>2018-02-10T15:09:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><p>Gradient descent is one of the methods for optimisation, especially when the objective function is convex. </p><ul><li><p>Gradient</p><p>The first question is, what is <strong>gradient</strong>? In mathematics, the gradient is a multivariate generalization of the derivative. </p><p>For a multivariate function $\varphi(x, y, z)$, the gradient can be written as the coordination form:</p><script type="math/tex; mode=display">\nabla \varphi(x, y, z)=\left(\frac{\partial\varphi}{\partial x},\frac{\partial\varphi}{\partial y},\frac{\partial\varphi}{\partial z}\right)</script><p>The direction of $\nabla\varphi $ is the orientation in which the <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> has the largest value and $|\nabla\varphi|$ is the value of that <a href="http://mathworld.wolfram.com/DirectionalDerivative.html" target="_blank" rel="external">directional derivative</a> [1]. If we consider a function of two variables, we can get a tangent plane at any point on the surface. There will be many directions along this plane,  and the direction with the largest slope is the gradient at this point. That is to say, the gradient at a particular point is the direction that the function value changes the fastest.</p></li><li><p>Gradient descent algorithm</p><p>Gradient descent algorithm can be used to find the local minimum of a differentiable  function $\varphi(\mathbf{\mu})$. After knowing what the gradient is, it’ll be easy to understand the gradient descent algorithm. Starting from an initial point $\mathbf{x}_0$ (usually this point is important), walk a small step $\eta$ (the step size is also important) along the opposite direction of the gradient, it will reach a new point, then restart from the new point. Repeat this procedure until the different in output after taking a step is smaller than the threshold $\eta$.<br>There are three parameters in this algorithm, the initial point, the step size, and the threshold respectively. Consider the sequence $\mathbf{\mu}_0, \mathbf{\mu}_1, \mathbf{\mu}_2, …$, such that</p><script type="math/tex; mode=display">\mathbf \mu_{t+1}=\mathbf \mu_t-\gamma\nabla\varphi(\mathbf \mu_t), t \geq 0</script></li><li><p>Gradient descent for linear regression</p><p>For linear regression, there are $n$ data points</p><script type="math/tex; mode=display">y_i=\mathbf x_i^T\mathbf w+b, i=1,2,...,n</script><p>The objective function to minimize is</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{n}\sum_{i=1}^{n}J_i(\theta)= \frac{1}{n}\sum_{i=1}^n J_i(\mathbf{w}, b)</script></li></ul><p>  Now, we can use the gradient descent algorithm to compute $\mathbf w$ and $b$, denote them as \theta$:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J(\mathbf \theta_t), t \geq 0</script><p>Code can be found <a href="https://github.com/lemelondeau/Algorithms/blob/master/SGD.ipynb" target="_blank" rel="external">here</a>.</p><h2 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h2><p>Many machine learning problems can be solved using gradient descent. However, when the dataset is very large, it will take a long time to update the  cost. Stochastic gradient descent (SGD) deals with this issue by updating the cost use only one sample each iteration:</p><script type="math/tex; mode=display">\mathbf \theta_{t+1}=\mathbf \theta_t-\gamma\nabla J_i(\mathbf \theta_t), t \geq 0</script><p>Gradient descent:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \frac{1}{n}\sum_{i=1}^{n}\nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>SGD:</p><script type="math/tex; mode=display">\text{g}_{t+1}\leftarrow \nabla J_i(\theta_{t})\\\theta_{t+1} \leftarrow \theta_{t}-\gamma \text{g}_{t+1}</script><p>The convergence of SGD is proven.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​    [1]. <a href="http://mathworld.wolfram.com/Gradient.html" target="_blank" rel="external">http://mathworld.wolfram.com/Gradient.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Gradient-Descent&quot;&gt;&lt;a href=&quot;#1-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;1. Gradient Descent&quot;&gt;&lt;/a&gt;1. Gradient Descent&lt;/h2&gt;&lt;p&gt;Grad
      
    
    </summary>
    
    
      <category term="algorithms" scheme="https://lemelondeau.github.io/tags/algorithms/"/>
    
  </entry>
  
</feed>
